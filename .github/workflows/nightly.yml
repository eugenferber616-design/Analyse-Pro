name: Nightly Data Pull & Cache (No LLM)

on:
  schedule:
    - cron: "30 0 * * *"   # tÃ¤glich 00:30 UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      WATCHLIST_STOCKS: watchlists/mylist.txt
      WATCHLIST_ETF:    watchlists/etf_sample.txt
      WINDOW_DAYS:      30
      CHEAP_MODE:       "true"
      FINNHUB_SLEEP_MS: "1300"

      # Options-Tuning
      OPTIONS_MAX_EXPIRIES: "all"
      OPTIONS_TOPK:        "5"
      RISK_FREE_RATE:      "0.045"
      HV_WIN_SHORT:        "20"
      HV_WIN_LONG:         "60"

      ENABLE_FUTURES: "false"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Init cache DB
        run: |
          mkdir -p data/cache data/earnings data/macro/fred data/macro/ecb data/market/stooq data/processed data/reports docs watchlists
          python - << 'PY'
          import os, sqlite3
          db_path = os.path.join('data','cache','cache.db')
          os.makedirs(os.path.dirname(db_path), exist_ok=True)
          con = sqlite3.connect(db_path)
          cur = con.cursor()
          cur.execute("""
          CREATE TABLE IF NOT EXISTS kv (
            k  TEXT PRIMARY KEY,
            v  TEXT NOT NULL,
            ts INTEGER DEFAULT (strftime('%s','now'))
          )""")
          cur.execute("CREATE INDEX IF NOT EXISTS ix_kv_ts ON kv(ts)")
          con.commit(); con.close()
          print("cache DB ready at", db_path)
          PY

      # â”€â”€ Secrets prÃ¼fen + Watchlists sichern
      - name: Prepare env and watchlists
        id: prep
        shell: bash
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          set -e
          TOKEN="${FINNHUB_TOKEN:-$FINNHUB_API_KEY}"
          if [ -z "$TOKEN" ]; then
            echo "âŒ FINNHUB_TOKEN/FINNHUB_API_KEY fehlt."
            exit 1
          fi
          echo "token_present=true" >> "$GITHUB_OUTPUT"

          if [ ! -f "$WATCHLIST_STOCKS" ]; then
            echo "âš ï¸  $WATCHLIST_STOCKS fehlt â€“ lege Fallback an."
            printf "symbol\nAAPL\nMSFT\nNVDA\nSPY\n" > watchlists/mylist.csv
            echo "WATCHLIST_STOCKS=watchlists/mylist.csv" >> "$GITHUB_ENV"
          fi
          echo "== Aktien Watchlist =="; head -n 10 "${WATCHLIST_STOCKS}" || true

          if [ ! -f "$WATCHLIST_ETF" ]; then
            printf "SPY\nQQQ\nIWM\nGLD\nHYG\nXLF\nXLK\nXLE\n" > watchlists/etf_sample.txt
          fi
          echo "== ETF Sample =="; head -n 10 "${WATCHLIST_ETF}" || true

      - name: Pull earnings CALENDAR (optional)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN || secrets.FINNHUB_API_KEY }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          mkdir -p data/earnings docs
          python scripts/fetch_earnings.py --watchlist "${WATCHLIST_STOCKS}" --window-days "${WINDOW_DAYS}"
          test -f docs/earnings_next.json && head -n 50 docs/earnings_next.json || true

      - name: Pull earnings RESULTS (EPS/Revenue)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN || secrets.FINNHUB_API_KEY }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FINNHUB_SLEEP_MS: ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          mkdir -p data/earnings/results
          python scripts/fetch_earnings_results.py --watchlist "${WATCHLIST_STOCKS}" --outdir data/earnings/results --limit 12
          test -f data/processed/earnings_results.csv && head -n 20 data/processed/earnings_results.csv || true

      - name: Pull ETF basics (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN || secrets.FINNHUB_API_KEY }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_etf_basics.py \
            --watchlist "${WATCHLIST_ETF}" \
            --out data/processed/etf_basics.csv \
            --errors data/reports/etf_errors.json || true
          head -n 30 data/processed/etf_basics.csv || true
          test -f data/reports/etf_errors.json && cat data/reports/etf_errors.json || true

      - name: Pull FUNDAMENTALS (profile2 + metrics)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN || secrets.FINNHUB_API_KEY }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FINNHUB_SLEEP_MS: ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          mkdir -p data/fundamentals
          python scripts/fetch_fundamentals.py --watchlist "${WATCHLIST_STOCKS}" --outdir data/fundamentals
          head -n 20 data/processed/fundamentals_core.csv || true

      # ========= COT 10y (robust, mit ENV fÃ¼r Timeout/Limit) =========
      - name: Pull COT (10y, Socrata) + compress
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
          COT_DATASET_ID:  gpe5-46if
          CFTC_API_BASE:   "https://publicreporting.cftc.gov/resource"
          COT_YEARS:       "10"
          COT_MARKETS_MODE: "ALL"
          COT_MARKETS_FILE: watchlists/cot_markets.txt
          SOC_TIMEOUT: "120"
          SOC_RETRIES: "6"
          SOC_BACKOFF: "1.6"
          SOC_LIMIT:   "25000"
        shell: bash
        run: |
          set -euo pipefail
          echo "== Socrata Smoke-Test =="
          curl -fsS --max-time 40 -G "${CFTC_API_BASE}/${COT_DATASET_ID}.json" \
            --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
            --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
            --data-urlencode '$limit=1' \
            -H "X-App-Token: ${CFTC_APP_TOKEN}" | tee /tmp/cftc_smoke_10y.json
          echo

          echo "== Pull 10y =="
          tries=3
          for i in $(seq 1 $tries); do
            if python -u scripts/fetch_cot_10y.py; then
              break
            fi
            echo "Fetcher failed (try $i/$tries) â€“ sleeping â€¦"
            sleep $((10 * i))
            if [ "$i" -eq "$tries" ]; then
              echo "âŒ fetch_cot_10y.py failed after $tries tries"
              test -f data/reports/cot_10y_report.json && cat data/reports/cot_10y_report.json || true
              exit 1
            fi
          done

          echo "== Preview =="
          head -n 5 data/processed/cot_10y.csv || true
          ls -lh data/processed/cot_10y.*

      # ========= COT (latest, robust) =========
      - name: Pull COT (CFTC Socrata)
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
          COT_DATASET_ID:  gpe5-46if
          COT_WEEKS:       "260"
          COT_MARKETS_MODE: "ALL"
          COT_MARKETS_FILE: watchlists/cot_markets.txt
          CFTC_API_BASE:   "https://publicreporting.cftc.gov/resource"
          SOC_TIMEOUT: "120"
          SOC_RETRIES: "6"
          SOC_BACKOFF: "1.6"
          SOC_LIMIT:   "25000"
        shell: bash
        run: |
          set -e
          if [ -z "${CFTC_APP_TOKEN}" ]; then
            echo "âŒ CFTC_APP_TOKEN fehlt. Bitte Secrets setzen."
            exit 1
          fi

          echo "== Socrata Smoke-Test (mit kleinem Retry) =="
          ok=0
          for i in 1 2 3; do
            if curl -fsS --max-time 40 -G "${CFTC_API_BASE}/${COT_DATASET_ID}.json" \
                 --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
                 --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
                 --data-urlencode '$limit=1' \
                 -H "X-App-Token: ${CFTC_APP_TOKEN}" \
              | tee /tmp/cftc_smoke.json ; then
              ok=1; break
            fi
            echo "â€¦Retry $i/3 nach Netzwerk/DNS-Fehlerâ€¦"; sleep 2
          done
          if [ "$ok" -ne 1 ]; then
            echo "âŒ Konnte Socrata nicht erreichen (DNS/Netz?). Abbruch."
            exit 1
          fi

          if grep -q '"permission_denied"' /tmp/cftc_smoke.json; then
            echo "âŒ CFTC_APP_TOKEN ungÃ¼ltig (permission_denied). Bitte Token im Secret aktualisieren."
            cat /tmp/cftc_smoke.json
            exit 1
          fi

          echo "== Fetcher starten (alle MÃ¤rkte, letzter Report) =="
          tries=3
          for i in $(seq 1 $tries); do
            if python -u scripts/fetch_cot_socrata.py; then
              break
            fi
            echo "Fetcher failed (try $i/$tries) â€“ sleeping â€¦"
            sleep $((10 * i))
            if [ "$i" -eq "$tries" ]; then
              echo "âŒ fetch_cot_socrata.py failed after $tries tries"
              test -f data/reports/cot_errors.json && cat data/reports/cot_errors.json || true
              exit 1
            fi
          done

          echo "== Vorschau (cot.csv / cot_summary.csv) =="
          head -n 5 data/processed/cot.csv || true
          head -n 5 data/processed/cot_summary.csv || true

      # ========= Macro (diverse FRED-Reihen fÃ¼r Heatmaps etc.) =========
      - name: Pull macro (FRED)
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        shell: bash
        run: |
          mkdir -p data/macro/fred
          python scripts/fetch_fred.py --out data/macro/fred
          test -f data/reports/fred_errors.json && cat data/reports/fred_errors.json || true

      # ========= FRED OAS (US & Euro) -> data/processed/fred_oas.csv =========
      - name: Pull FRED OAS (US & Euro)
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          python scripts/fetch_fred_oas.py --api-key "${FRED_API_KEY}"
          tail -n 5 data/processed/fred_oas.csv || true
          test -f data/reports/fred_errors.json && cat data/reports/fred_errors.json || true

      # ========= ECB (neue API) â€“ Beispiele: EXR & CISS =========
      - name: Pull ECB (new API)
        run: |
          python scripts/fetch_ecb.py --since 2010-01-01
          test -f data/reports/ecb_errors.json && cat data/reports/ecb_errors.json || true
          ls -lh data/macro/ecb/* || true

      # ========= Stooq Quotes (EU & US aus Watchlist) =========
      - name: Pull Stooq quotes (EU & US aus Watchlist)
        run: |
          mkdir -p data/market/stooq
          python scripts/fetch_stooq.py --watchlist "${WATCHLIST_STOCKS}" || true
          find data/market/stooq -type f | head -n 10 || true

      # ========= CDS Proxy v2 (nutzt US/EU OAS + Fundamentals + Stooq HV20) =========
      - name: Build CDS proxy per symbol
        env:
          WATCHLIST_STOCKS: ${{ env.WATCHLIST_STOCKS }}
        shell: bash
        run: |
          python scripts/build_cds_proxy_v2.py
          tail -n 10 data/processed/cds_proxy.csv || true
          test -f data/reports/cds_proxy_report.json && cat data/reports/cds_proxy_report.json || true

      # ========= EU Ampel (ECB/Stooq/CDS Previews) =========
      - name: EU macro checks (Ampel)
        run: |
          python scripts/build_eu_ampel.py
          echo "== ECB preview ==" && head -n 20 data/reports/eu_checks/ecb_preview.txt || true
          echo "== Stooq preview ==" && head -n 20 data/reports/eu_checks/stooq_preview.txt || true
          echo "== CDS proxy (EU) ==" && head -n 20 data/reports/eu_checks/cds_proxy_preview.txt || true

      - name: Validate processed outputs
        shell: bash
        run: |
          python - << 'PY'
          import os, json
          def count_rows(p):
              if not os.path.exists(p) or os.path.getsize(p)==0:
                  return 0
              with open(p, encoding="utf-8") as f:
                  return max(0, sum(1 for _ in f) - 1)
          checks = {
              "fundamentals_core.csv":      count_rows("data/processed/fundamentals_core.csv"),
              "earnings_results.csv":       count_rows("data/processed/earnings_results.csv"),
              "etf_basics.csv":             count_rows("data/processed/etf_basics.csv"),
              "fred_oas.csv":               count_rows("data/processed/fred_oas.csv"),
              "cot_summary.csv":            count_rows("data/processed/cot_summary.csv"),
              "options_oi_summary.csv":     count_rows("data/processed/options_oi_summary.csv"),
              "options_oi_by_expiry.csv":   count_rows("data/processed/options_oi_by_expiry.csv"),
              "options_oi_totals.csv":      count_rows("data/processed/options_oi_totals.csv"),
              "cds_proxy.csv":              count_rows("data/processed/cds_proxy.csv"),
          }
          lamp = lambda n: "ðŸŸ¢" if n>0 else "ðŸ”´"
          print("\n=== QA Summary ===")
          for k,v in checks.items():
              print(f"{lamp(v)} {k}: {v} rows")
          os.makedirs("data/reports", exist_ok=True)
          json.dump(checks, open("data/reports/qa_summary.json","w"), indent=2)
          PY
          cat data/reports/qa_summary.json || true

      - name: List written files (sizes)
        shell: bash
        run: |
          echo "== data tree =="; find data -type f -printf "%p\t%k KB\n" | sort || true
          echo "== docs tree =="; find docs -type f -printf "%p\t%k KB\n" | sort || true
          python - << 'PY'
          import json, time, os
          info = {
              "batch": 100, "window_days": int(os.getenv("WINDOW_DAYS","30")),
              "cheap_mode": os.getenv("CHEAP_MODE","true")=="true",
              "watchlists": {
                  "stocks": os.getenv("WATCHLIST_STOCKS"),
                  "etf": os.getenv("WATCHLIST_ETF"),
              },
              "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
          }
          os.makedirs("data/reports", exist_ok=True)
          json.dump(info, open("data/reports/last_run.json","w"), indent=2)
          print(json.dumps(info, indent=2))
          PY
          echo "== last_run.json ==" && cat data/reports/last_run.json

      - name: Fail if outputs empty
        shell: bash
        run: |
          cnt=$(find data -type f -size +0c | wc -l || true)
          dcnt=$(find docs -type f -size +0c | wc -l || true)
          echo "Non-empty files in data: $cnt ; in docs: $dcnt"
          if [ "$cnt" -eq 0 ] && [ "$dcnt" -eq 0 ]; then
            echo "No non-empty files produced."
            exit 1
          fi

      - name: Commit updated cache & reports
        shell: bash
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/ docs/ ':!data/processed/*.gz' ':!data/processed/*.zip' || true
          git commit -m "Nightly cache update (no LLM)" || echo "Nothing to commit"
          git push

      - name: Upload data artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: data-bundle
          path: |
            data/**
            docs/**

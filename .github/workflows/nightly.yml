name: Nightly Data Pull and Cache (No LLM)

on:
  schedule:
    - cron: "30 0 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    env:
      WATCHLIST_STOCKS: watchlists/mylist.txt
      WATCHLIST_ETF:    watchlists/etf_sample.txt

      PRICES_DAYS:      "750"
      FINNHUB_SLEEP_MS: "1300"

      WINDOW_DAYS:          "30"
      OPTIONS_MAX_EXPIRIES: "all"
      OPTIONS_TOPK:         "5"
      RISK_FREE_RATE:       "0.045"
      HV_WIN_SHORT:         "20"
      HV_WIN_LONG:          "60"
      HV_WIN_10:            "10"
      HV_WIN_30:            "30"

      ENABLE_FUTURES: "false"

      CFTC_API_BASE:         "https://publicreporting.cftc.gov/resource"
      COT_DISAGG_DATASET_ID: "kh3c-gbw2"
      COT_TFF_DATASET_ID:    "yw9f-hn96"
      COT_YEARS:             "20"
      COT_MARKETS_MODE:      "SMART"
      COT_MARKETS_FILE:      watchlists/cot_markets.txt
      SOC_TIMEOUT:           "120"
      SOC_RETRIES:           "6"
      SOC_BACKOFF:           "1.6"
      SOC_LIMIT:             "25000"

      PYTHONPATH: "."

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install --upgrade pyarrow fastparquet
          sudo apt-get update -y && sudo apt-get install -y jq

      - name: Ensure folders + init cache DB
        run: |
          mkdir -p data/{cache,earnings,fundamentals,macro,prices,processed,reports} docs watchlists
          python - << 'PY'
          import os, sqlite3
          p = os.path.join('data','cache','cache.db')
          os.makedirs(os.path.dirname(p), exist_ok=True)
          con=sqlite3.connect(p); cur=con.cursor()
          cur.execute("""CREATE TABLE IF NOT EXISTS kv (
            k TEXT PRIMARY KEY, v TEXT NOT NULL, ts INTEGER DEFAULT (strftime('%s','now'))
          )""")
          cur.execute("CREATE INDEX IF NOT EXISTS ix_kv_ts ON kv(ts)")
          con.commit(); con.close()
          PY

      - name: Prepare env + watchlists
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          set -e
          FINN="${FINNHUB_TOKEN:-$FINNHUB_API_KEY}"
          if [ -z "$FINN" ]; then echo "FINNHUB key fehlt"; exit 1; fi
          echo "FINNHUB_KEY=$FINN" >> "$GITHUB_ENV"
          test -f "$WATCHLIST_STOCKS" || printf "AAPL\nMSFT\nNVDA\nSPY\n" > watchlists/mylist.txt
          test -f "$WATCHLIST_ETF"    || printf "SPY\nQQQ\nIWM\nGLD\nHYG\nXLF\nXLK\nXLE\n" > watchlists/etf_sample.txt
          cat "$WATCHLIST_STOCKS" "$WATCHLIST_ETF" | sed 's/\r$//' | sed '/^\s*$/d' | sort -u > /tmp/options_watchlist.txt

      # ---------- Earnings ----------
      - name: Pull earnings CALENDAR
        run: |
          mkdir -p data/earnings docs
          python scripts/fetch_earnings.py --watchlist "${WATCHLIST_STOCKS}" --window-days "${WINDOW_DAYS}" --key "${FINNHUB_KEY}"
          test -f docs/earnings_next.json && head -n 50 docs/earnings_next.json || true

      - name: Pull earnings RESULTS
        env:
          FINNHUB_SLEEP_MS: ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          mkdir -p data/earnings/results
          python scripts/fetch_earnings_results.py --watchlist "${WATCHLIST_STOCKS}" --outdir data/earnings/results --limit 12 --key "${FINNHUB_KEY}" --sleep-ms "${FINNHUB_SLEEP_MS}"
          test -f data/processed/earnings_results.csv && head -n 20 data/processed/earnings_results.csv || true

      # ---------- ETF Basics / Fundamentals ----------
      - name: Pull ETF basics
        run: |
          python config/fetch_etf_basics.py --watchlist "${WATCHLIST_ETF}" \
            --out data/processed/etf_basics.csv \
            --errors data/reports/etf_errors.json --key "${FINNHUB_KEY}" || true
          head -n 30 data/processed/etf_basics.csv || true
          test -f data/reports/etf_errors.json && cat data/reports/etf_errors.json || true

      - name: Pull FUNDAMENTALS (profile2 + metrics)
        run: |
          mkdir -p data/fundamentals
          python scripts/fetch_fundamentals.py --watchlist "${WATCHLIST_STOCKS}" --finnhub_key "${FINNHUB_KEY}" --sleep_ms 150
          head -n 20 data/processed/fundamentals_core.csv || true

      - name: Fetch Options OI + IV/HV summary
        run: |
          python scripts/fetch_options_oi.py --watchlist /tmp/options_watchlist.txt
          head -n 40 data/processed/options_oi_summary.csv || true
          head -n 40 data/processed/options_oi_by_expiry.csv || true
          head -n 40 data/processed/options_oi_totals.csv || true

      - name: "Post-build: max OI strike per symbol"
        run: |
          python scripts/post_build_strike_max.py
          head -n 20 data/processed/options_oi_strike_max.csv || true

      - name: Build options BY_STRIKE (aus Summary-TopStrikes)
        run: |
          python scripts/build_options_by_strike.py
          head -n 40 data/processed/options_oi_by_strike.csv || true

      - name: Build options SIGNALS (pro Symbol)
        run: |
          python scripts/build_options_signals.py
          head -n 40 data/processed/options_signals.csv || true


      # ---------- COT 20y ----------
      - name: Pull COT 20y – Disaggregated
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        run: |
          curl -fsS -G "${CFTC_API_BASE}/${COT_DISAGG_DATASET_ID}.json" \
            --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
            --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
            --data-urlencode '$limit=1' \
            -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq . || true
          python -u scripts/fetch_cot_20y.py --dataset "${COT_DISAGG_DATASET_ID}" --out "data/processed/cot_20y_disagg.csv.gz"
          zcat data/processed/cot_20y_disagg.csv.gz | head -n 2 || true

      - name: Pull COT 20y – TFF
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        run: |
          curl -fsS -G "${CFTC_API_BASE}/${COT_TFF_DATASET_ID}.json" \
            --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
            --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
            --data-urlencode '$limit=1' \
            -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq . || true
          python -u scripts/fetch_cot_20y.py --dataset "${COT_TFF_DATASET_ID}" --out "data/processed/cot_20y_tff.csv.gz"
          zcat data/processed/cot_20y_tff.csv.gz | head -n 2 || true

      # ---------- Stooq HV ----------
      - name: Pull Stooq quotes → HV summary → purge raws
        env:
          HV_WIN_SHORT: ${{ env.HV_WIN_SHORT }}
          HV_WIN_LONG:  ${{ env.HV_WIN_LONG }}
        run: |
          set -e
          mkdir -p data/market/stooq data/processed
          python scripts/fetch_stooq.py --watchlist "${WATCHLIST_STOCKS}" --days 365 || true
          python scripts/build_hv_summary.py || python scripts/build_hv_summary.py || true
          rm -f data/market/stooq/*.csv || true

      # ---------- Komprimieren ----------
      - name: Compress processed outputs to .gz
        env:
          COMPRESS_MIN_KB: "50"
        run: |
          set -e
          find data/processed -type f \( -name "*.csv" -o -name "*.json" -o -name "*.ndjson" \) \
               -size +${COMPRESS_MIN_KB}k -print0 | xargs -0 -I{} gzip -f "{}" || true
          if [ -d docs ]; then
            find docs -type f -name "*.json" -size +${COMPRESS_MIN_KB}k -print0 | xargs -0 -I{} gzip -f "{}" || true
          fi
          echo "== processed after gzip =="; find data/processed -maxdepth 1 -type f -printf "%p\t%k KB\n" | sort || true

      # ---------- R2 Upload ----------
      - name: Sync .gz (and docs) to R2
        env:
          CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
          CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}
          CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
          CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
        run: |
          set -e
          if [ -z "${CF_R2_ACCESS_KEY_ID}" ] || [ -z "${CF_R2_SECRET_ACCESS_KEY}" ] || \
             [ -z "${CF_R2_ENDPOINT}" ] || [ -z "${CF_R2_BUCKET}" ]; then
            echo "R2 nicht konfiguriert – Step übersprungen."; exit 0
          fi
          curl -fsSL https://rclone.org/install.sh | sudo bash
          cat > rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${CF_R2_ACCESS_KEY_ID}
          secret_access_key = ${CF_R2_SECRET_ACCESS_KEY}
          endpoint = ${CF_R2_ENDPOINT}
          acl = private
          force_path_style = true
          EOF
          rclone sync data/processed "r2:${CF_R2_BUCKET}/data/processed" \
            --config rclone.conf --fast-list --checksum --update \
            --include "*.csv.gz" --include "*.json.gz" --exclude "*"
          rclone sync docs "r2:${CF_R2_BUCKET}/docs" \
            --config rclone.conf --fast-list --checksum --update
          rclone ls "r2:${CF_R2_BUCKET}/data/processed" --config rclone.conf | head -n 50 || true

      # ---------- Repo schlank halten ----------
      - name: Purge heavy local files before commit
        run: |
          set -e
          find data/processed -type f ! -name ".gitkeep" -delete || true

      # ---------- Manifest + Commit ----------
      - name: Build datasets manifest (docs/datasets.json)
        env:
          PUBLIC_BASE: ${{ vars.CF_R2_PUBLIC_BASE }}
        run: |
          mkdir -p docs
          python - << 'PY'
          import json, os, pathlib
          base = os.getenv("PUBLIC_BASE", "https://pub-CHANGE-ME.r2.dev")
          manifest = {
            "base": base,
            "datasets": {
              "cot_20y_disagg": "/data/processed/cot_20y_disagg.csv.gz",
              "cot_20y_tff":    "/data/processed/cot_20y_tff.csv.gz",
              "options_by_expiry": "/data/processed/options_oi_by_expiry.csv.gz",
              "options_totals":    "/data/processed/options_oi_totals.csv.gz",
              "options_by_strike": "/data/processed/options_oi_by_strike.csv.gz",
              "options_strike_max":"/data/processed/options_oi_strike_max.csv.gz"
            }
          }
          pathlib.Path("docs").mkdir(parents=True, exist_ok=True)
          open("docs/datasets.json","w",encoding="utf-8").write(json.dumps(manifest,indent=2,ensure_ascii=False))
          print("datasets.json base =", base)
          PY

      - name: Commit updated cache & reports (no big files)
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/ docs/ \
            ':!data/processed/*.csv' ':!data/processed/*.csv.gz' \
            ':!data/processed/*.parquet' ':!data/processed/*.zip' ':!data/processed/*.json.gz' || true
          git commit -m "Nightly cache update (no LLM, COT Disagg+TFF, gz to R2)" || echo "Nothing to commit"
          git push

      - name: Upload reports & docs (artifact)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: reports-and-docs
          path: |
            data/reports/**
            docs/**

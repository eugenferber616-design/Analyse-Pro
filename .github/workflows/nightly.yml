# .github/workflows/analyze_signale.yml
name: Nightly Data Pull and Cache (No LLM)

on:
  schedule:
    - cron: "30 0 * * *"   # täglich 00:30 UTC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      WATCHLIST_STOCKS: watchlists/mylist.txt
      WATCHLIST_ETF:    watchlists/etf_sample.txt
      FORKAST_SUBDIR:   "prompter"   # unbenutzt, kann bleiben

      # Kurse/History
      PRICES_DAYS:      "750"
      FINNHUB_SLEEP_MS: "1300"

      # Earnings / Optionen / HV
      WINDOW_DAYS:          "30"
      OPTIONS_MAX_EXPIRIES: "all"
      OPTIONS_TOPK:         "5"
      RISK_FREE_RATE:       "0.045"
      HV_WIN_SHORT:         "20"
      HV_WIN_LONG:          "60"
      HV_WIN_10:            "10"
      HV_WIN_30:            "30"

      ENABLE_FUTURES: "false"

      # COT – Socrata Basis
      CFTC_API_BASE:         "https://publicreporting.cftc.gov/resource"
      COT_DISAGG_DATASET_ID: "kh3c-gbw2"
      COT_TFF_DATASET_ID:    "yw9f-hn96"
      COT_YEARS:             "20"
      COT_MARKETS_MODE:      "SMART"        # "ALL" | "FILE" | "LIST" | "SMART"
      COT_MARKETS_FILE:      watchlists/cot_markets.txt
      SOC_TIMEOUT:           "120"
      SOC_RETRIES:           "8"
      SOC_BACKOFF:           "2.0"
      SOC_LIMIT:             "20000"

      # FRED
      FRED_START: "2003-01-01"

      # lokale Module auffindbar
      PYTHONPATH: "."

      # Secrets in env spiegeln → if: kann env.* prüfen
      CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
      CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
      CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
      CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install jq + rclone
        run: |
          sudo apt-get update
          sudo apt-get install -y jq rclone

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Zusätzliche Pakete, die in Inline-Skripten genutzt werden
          pip install --upgrade yfinance pandas_datareader requests
          # parquet-Engines optional
          pip install --upgrade pyarrow fastparquet matplotlib

      - name: Init cache DB
        shell: bash
        run: |
          mkdir -p data/cache data/earnings data/macro/fred data/macro/ecb data/market/stooq data/market/core data/processed data/reports data/history docs watchlists scripts
          python - <<'PY'
          import os, sqlite3
          db_path = os.path.join('data','cache','cache.db')
          os.makedirs(os.path.dirname(db_path), exist_ok=True)
          con = sqlite3.connect(db_path)
          cur = con.cursor()
          cur.execute("""
          CREATE TABLE IF NOT EXISTS kv (
            k  TEXT PRIMARY KEY,
            v  TEXT NOT NULL,
            ts INTEGER DEFAULT (strftime('%s','now'))
          )""")
          cur.execute("CREATE INDEX IF NOT EXISTS ix_kv_ts ON kv(ts)")
          con.commit(); con.close()
          print("cache DB ready at", db_path)
          PY

      - name: Prepare env and watchlists
        id: prep
        shell: bash
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          set -e
          TOKEN="${FINNHUB_TOKEN:-$FINNHUB_API_KEY}"
          if [ -z "$TOKEN" ]; then
            echo "❌ FINNHUB_TOKEN/FINNHUB_API_KEY fehlt."
            exit 1
          fi
          echo "token_present=true" >> "$GITHUB_OUTPUT"

          # Aktien-Watchlist normalisieren
          if [ ! -f "$WATCHLIST_STOCKS" ]; then
            printf "symbol\nAAPL\nMSFT\nNVDA\nSPY\n" > watchlists/mylist.csv
            echo "WATCHLIST_STOCKS=watchlists/mylist.csv" >> "$GITHUB_ENV"
          fi
          echo "== Aktien Watchlist =="; head -n 10 "${WATCHLIST_STOCKS}" || true

          # ETF-Watchlist normalisieren
          if [ ! -f "$WATCHLIST_ETF" ]; then
            printf "SPY\nQQQ\nIWM\nGLD\nHYG\nXLF\nXLK\nXLE\n" > watchlists/etf_sample.txt
          fi
          echo "== ETF Sample =="; head -n 10 "${WATCHLIST_ETF}" || true

      # ---------------- Earnings / Fundamentals / Optionen -------------------
      - name: Pull earnings CALENDAR (optional)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          mkdir -p data/earnings docs
          python scripts/fetch_earnings.py --watchlist "${WATCHLIST_STOCKS}" --window-days "${WINDOW_DAYS}"
          test -f docs/earnings_next.json && head -n 50 docs/earnings_next.json || true

      - name: Pull earnings RESULTS (historisch, robust)
        env:
          FINNHUB_TOKEN:        ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY:      ${{ secrets.FINNHUB_API_KEY }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          SEC_USER_AGENT:       ${{ secrets.SEC_USER_AGENT }}
          SIMFIN_API_KEY:       ${{ secrets.SIMFIN_API_KEY }}
          FINNHUB_SLEEP_MS:     ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          python scripts/fetch_earnings_results.py --watchlist "${WATCHLIST_STOCKS}" --limit 40
          test -f data/processed/earnings_results.csv && head -n 20 data/processed/earnings_results.csv || true

      - name: Pull ETF basics (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_etf_basics.py --watchlist "${WATCHLIST_ETF}" \
            --out data/processed/etf_basics.csv \
            --errors data/reports/etf_errors.json || true
          head -n 30 data/processed/etf_basics.csv || true
          test -f data/reports/etf_errors.json && cat data/reports/etf_errors.json || true

      - name: Pull FUNDAMENTALS (profile2 + metrics, mit Fallbacks)
        env:
          FINNHUB_API_KEY:       ${{ secrets.FINNHUB_API_KEY }}
          FINNHUB_TOKEN:         ${{ secrets.FINNHUB_TOKEN }}
          ALPHAVANTAGE_API_KEY:  ${{ secrets.ALPHAVANTAGE_API_KEY }}
          SIMFIN_API_KEY:        ${{ secrets.SIMFIN_API_KEY }}
        run: |
          python scripts/fetch_fundamentals.py --watchlist "${WATCHLIST_STOCKS}" --sleep_ms 200 || true
          head -n 20 data/processed/fundamentals_core.csv || true

      - name: Fetch Options OI + IV/HV summary (Stocks + ETFs)
        shell: bash
        env:
          OPTIONS_MAX_EXPIRIES: ${{ env.OPTIONS_MAX_EXPIRIES }}
          OPTIONS_TOPK:         ${{ env.OPTIONS_TOPK }}
          HV_WIN_SHORT:         ${{ env.HV_WIN_SHORT }}
          HV_WIN_LONG:          ${{ env.HV_WIN_LONG }}
          HV_WIN_10:            ${{ env.HV_WIN_10 }}
          HV_WIN_30:            ${{ env.HV_WIN_30 }}
        run: |
          set -e
          tmp=/tmp/options_watchlist.txt
          {
            if [ -f "${WATCHLIST_STOCKS}" ]; then
              # CSV: erste Spalte Ticker
              tail -n +2 "${WATCHLIST_STOCKS}" | cut -d',' -f1 | tr -d '\r'
            fi
            if [ -f "${WATCHLIST_ETF}" ]; then
              if head -n1 "${WATCHLIST_ETF}" | grep -qi '^symbol'; then
                tail -n +2 "${WATCHLIST_ETF}" | tr -d '\r'
              else
                cat "${WATCHLIST_ETF}" | tr -d '\r'
              fi
            fi
          } | awk 'NF>0' | sort -u > "$tmp"

          echo "== Options Watchlist =="; cat "$tmp" | head -n 40

          python scripts/fetch_options_oi.py --watchlist "$tmp"
          test -s data/processed/options_oi_summary.csv      || (echo "options_oi_summary leer!" && exit 1)
          test -s data/processed/options_oi_by_expiry.csv    || (echo "options_oi_by_expiry leer!" && exit 1)
          test -s data/processed/options_oi_totals.csv       || (echo "options_oi_totals leer!" && exit 1)

          head -n 40 data/processed/options_oi_summary.csv   || true
          head -n 40 data/processed/options_oi_by_expiry.csv || true
          head -n 40 data/processed/options_oi_totals.csv    || true

      - name: Build max OI strike per symbol
        run: |
          python scripts/post_build_strike_max.py
          test -s data/processed/options_oi_strike_max.csv || (echo "options_oi_strike_max leer!" && exit 1)
          head -n 20 data/processed/options_oi_strike_max.csv || true

      - name: Build options BY_STRIKE (aus Summary-TopStrikes)
        run: |
          python scripts/build_options_by_strike.py
          test -s data/processed/options_oi_by_strike.csv || (echo "options_oi_by_strike leer!" && exit 1)
          head -n 40 data/processed/options_oi_by_strike.csv || true

      - name: Build options SIGNALS (pro Symbol)
        run: |
          python scripts/build_options_signals.py
          test -s data/processed/options_signals.csv || (echo "options_signals leer!" && exit 1)
          head -n 40 data/processed/options_signals.csv || true

      - name: Build direction_signal (kompakte Overlay-Datei)
        shell: bash
        run: |
          set -euo pipefail

          # Hauptskript aus scripts/ nutzen
          python scripts/build_direction_signal.py

          # Robust: akzeptiere .csv **oder** .csv.gz als Output
          if [ -s data/processed/direction_signal.csv ]; then
            echo "✔ direction_signal.csv gefunden"
            head -n 40 data/processed/direction_signal.csv || true
          elif [ -s data/processed/direction_signal.csv.gz ]; then
            echo "✔ direction_signal.csv.gz gefunden"
            zcat data/processed/direction_signal.csv.gz | head -n 40 || true
          else
            echo "❌ direction_signal leer oder fehlt!" >&2
            exit 1
          fi


          # Robust: akzeptiere .csv oder .csv.gz
          if [ -s data/processed/direction_signal.csv ]; then
            echo "✔ direction_signal.csv gefunden"
            head -n 40 data/processed/direction_signal.csv || true
          elif [ -s data/processed/direction_signal.csv.gz ]; then
            echo "✔ direction_signal.csv.gz gefunden"
            zcat data/processed/direction_signal.csv.gz | head -n 40 || true
          else
            echo "❌ direction_signal leer!" >&2
            exit 1
          fi



      # -------------------- PREISDATEN (OHLC) FÜR WATCHLIST -------------------
      - name: Fetch OHLC prices for watchlist (yfinance)
        run: |
          python - <<'PY'
          import os, pandas as pd, yfinance as yf
          from datetime import datetime, timedelta

          days = int(os.getenv("PRICES_DAYS","750"))
          os.makedirs("data/prices", exist_ok=True)

          def read_list(p):
              if not p or not os.path.exists(p): return []
              df = pd.read_csv(p)
              col = "symbol" if "symbol" in df.columns else df.columns[0]
              return [str(s).strip().upper() for s in df[col].dropna().tolist()]

          wl = sorted(set(read_list(os.getenv("WATCHLIST_STOCKS")) + read_list(os.getenv("WATCHLIST_ETF"))))
          start = (datetime.utcnow() - timedelta(days=days*1.2)).strftime("%Y-%m-%d")

          for sym in wl:
              try:
                  df = yf.download(sym, start=start, auto_adjust=False, progress=False)
                  if df is None or df.empty:
                      continue
                  df.index = df.index.tz_localize(None)
                  df.reset_index().rename(columns={"Date":"date","Open":"open","High":"high","Low":"low","Close":"close","Adj Close":"adj_close","Volume":"volume"}) \
                    .to_csv(f"data/prices/{sym}.csv", index=False)
              except Exception:
                  pass

          print("saved", len([f for f in os.listdir('data/prices') if f.endswith('.csv')]), "files to data/prices")
          PY

      # -------------------- SHORT INTEREST (Rohdaten) -------------------------
      - name: Create scripts/fetch_short_interest.py
        run: |
          cat > scripts/fetch_short_interest.py <<'PY'
          import os, time, json, pandas as pd, requests
          from datetime import datetime, timedelta
          BASE="https://finnhub.io/api/v1"
          TOKEN=os.getenv("FINNHUB_TOKEN") or os.getenv("FINNHUB_API_KEY")

          def q(url, params):
              params = dict(params or {})
              if TOKEN: params["token"]=TOKEN
              for i in range(3):
                  try:
                      r = requests.get(url, params=params, timeout=30)
                      if r.status_code==200: return r.json()
                  except Exception: pass
                  time.sleep(1+2*i)
              return None

          def read_symbols(p):
              if not p or not os.path.exists(p): return []
              df=pd.read_csv(p); col="symbol" if "symbol" in df.columns else df.columns[0]
              return [str(x).strip().upper() for x in df[col].dropna().tolist()]

          def main():
              wl=set(read_symbols(os.getenv("WATCHLIST_STOCKS"))+read_symbols(os.getenv("WATCHLIST_ETF")))
              out="data/processed/short_interest.csv"
              os.makedirs("data/processed", exist_ok=True)
              rows=[]
              fr=(datetime.utcnow()-timedelta(days=400)).strftime("%Y-%m-%d")
              to=datetime.utcnow().strftime("%Y-%m-%d")
              for sym in sorted(wl):
                  si = q(BASE+"/stock/short-interest", {"symbol":sym, "from":fr, "to":to}) or {}
                  si_data = si.get("data") if isinstance(si, dict) else None
                  si_last = (si_data or [{}])[-1] if si_data else {}
                  br = q(BASE+"/stock/borrow", {"symbol":sym}) or {}
                  br_last = (br.get("data") or [{}])[-1] if isinstance(br, dict) else {}
                  fl = q(BASE+"/stock/float", {"symbol":sym}) or {}
                  float_sh = fl.get("floatShares") if isinstance(fl, dict) else None
                  si_sh = si_last.get("shortInterest") or si_last.get("short_interest")
                  si_dt = si_last.get("date") or si_last.get("t")
                  pct_float = None
                  try:
                      if float_sh and si_sh: pct_float = 100.0*float(si_sh)/float(float_sh)
                  except Exception: pct_float=None
                  rows.append({
                      "symbol": sym,
                      "si_date": si_dt,
                      "si_shares": si_sh,
                      "float_shares": float_sh,
                      "si_pct_float": pct_float,
                      "borrow_date": br_last.get("date") or br_last.get("t"),
                      "borrow_rate": br_last.get("rate") or br_last.get("feeRate") or br_last.get("fr"),
                      "borrow_avail": br_last.get("available") or br_last.get("shares"),
                  })
              pd.DataFrame(rows).to_csv(out, index=False)
              print("wrote", out, "rows", len(rows))

          if __name__=="__main__": main()
          PY

      - name: Fetch short interest (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_short_interest.py
          head -n 10 data/processed/short_interest.csv || true

      # -------------------- INSIDER-TRANSAKTIONEN (Rohdaten) -----------------
      - name: Create scripts/fetch_insider_tx.py
        run: |
          cat > scripts/fetch_insider_tx.py <<'PY'
          import os, time, pandas as pd, requests
          BASE="https://finnhub.io/api/v1"
          TOKEN=os.getenv("FINNHUB_TOKEN") or os.getenv("FINNHUB_API_KEY")

          def q(url, params):
              params=dict(params or {}); 
              if TOKEN: params["token"]=TOKEN
              for i in range(3):
                  try:
                      r=requests.get(url, params=params, timeout=30)
                      if r.status_code==200: return r.json()
                  except Exception: pass
                  time.sleep(1+2*i)
              return None

          def read_symbols(p):
              if not p or not os.path.exists(p): return []
              df=pd.read_csv(p); col="symbol" if "symbol" in df.columns else df.columns[0]
              return [str(x).strip().upper() for x in df[col].dropna().tolist()]

          def main():
              wl=set(read_symbols(os.getenv("WATCHLIST_STOCKS"))+read_symbols(os.getenv("WATCHLIST_ETF")))
              out="data/processed/insider_tx.csv"
              rows=[]
              for sym in sorted(wl):
                  r=q(BASE+"/stock/insider-transactions", {"symbol":sym, "from":"2018-01-01"})
                  data = (r or {}).get("data") or []
                  for it in data:
                      it = it or {}
                      rows.append({
                          "symbol": sym,
                          "name": it.get("name"),
                          "position": it.get("position"),
                          "transaction_code": it.get("transactionCode") or it.get("code"),
                          "share": it.get("share"),
                          "change": it.get("change"),
                          "transaction_price": it.get("transactionPrice") or it.get("price"),
                          "filing_date": it.get("filingDate"),
                          "transaction_date": it.get("transactionDate"),
                      })
              pd.DataFrame(rows).to_csv(out, index=False)
              print("wrote", out, "rows", len(rows))

          if __name__=="__main__": main()
          PY

      - name: Fetch insider transactions (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_insider_tx.py
          head -n 10 data/processed/insider_tx.csv || true

      # -------------------- FUND-OWNERSHIP (Institutionelle) -----------------
      - name: Create scripts/fetch_fund_ownership.py
        run: |
          cat > scripts/fetch_fund_ownership.py <<'PY'
          import os, time, pandas as pd, requests
          BASE="https://finnhub.io/api/v1"
          TOKEN=os.getenv("FINNHUB_TOKEN") or os.getenv("FINNHUB_API_KEY")

          def q(url, params):
              params=dict(params or {}); 
              if TOKEN: params["token"]=TOKEN
              for i in range(3):
                  try:
                      r=requests.get(url, params=params, timeout=30)
                      if r.status_code==200: return r.json()
                  except Exception: pass
                  time.sleep(1+2*i)
              return None

          def read_symbols(p):
              if not p or not os.path.exists(p): return []
              df=pd.read_csv(p); col="symbol" if "symbol" in df.columns else df.columns[0]
              return [str(x).strip().upper() for x in df[col].dropna().tolist()]

          def main():
              wl=set(read_symbols(os.getenv("WATCHLIST_STOCKS"))+read_symbols(os.getenv("WATCHLIST_ETF")))
              out="data/processed/fund_ownership.csv"
              rows=[]
              for sym in sorted(wl):
                  r=q(BASE+"/stock/fund-ownership", {"symbol":sym, "limit":200})
                  data = (r or {}).get("ownership") or []
                  for it in data:
                      rows.append({
                          "symbol": sym,
                          "owner": it.get("owner") or it.get("holder"),
                          "percent": it.get("percent"),
                          "reported_date": it.get("reportedDate") or it.get("reportDate"),
                          "position": it.get("position"),
                          "avg_price": it.get("avgPrice"),
                      })
              pd.DataFrame(rows).to_csv(out, index=False)
              print("wrote", out, "rows", len(rows))

          if __name__=="__main__": main()
          PY

      - name: Fetch fund ownership (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_fund_ownership.py
          head -n 10 data/processed/fund_ownership.csv || true

      # -------------------- FINANZBERICHTE (Quarterly Timeseries) ------------
      - name: Create scripts/fetch_financials_ts.py
        run: |
          cat > scripts/fetch_financials_ts.py <<'PY'
          import os, time, pandas as pd, requests
          BASE="https://finnhub.io/api/v1"
          TOKEN=os.getenv("FINNHUB_TOKEN") or os.getenv("FINNHUB_API_KEY")
          KEYS = {
              "ic": ["revenue","grossProfit","operatingIncome","netIncome","eps","epsDiluted","researchAndDevelopment"],
              "bs": ["totalAssets","totalLiabilities","totalDebt","cashAndCashEquivalents","shareholdersEquity","inventory"],
              "cf": ["operatingCashFlow","capitalExpenditure","freeCashFlow","depreciationAndAmortization"]
          }

          def q(url, params):
              params=dict(params or {}); 
              if TOKEN: params["token"]=TOKEN
              for i in range(3):
                  try:
                      r=requests.get(url, params=params, timeout=40)
                      if r.status_code==200: return r.json()
                  except Exception: pass
                  time.sleep(1+2*i)
              return None

          def read_symbols(p):
              if not p or not os.path.exists(p): return []
              df=pd.read_csv(p); col="symbol" if "symbol" in df.columns else df.columns[0]
              return [str(x).strip().upper() for x in df[col].dropna().tolist()]

          def pull_statement(sym, stmt):
              r=q(BASE+"/stock/financials", {"symbol":sym,"statement":stmt,"freq":"quarterly"}) or {}
              data=r.get("data") or []
              rows=[]
              for it in data:
                  period=it.get("period") or it.get("calendarDate") or it.get("reportDate")
                  for k in KEYS[stmt]:
                      if k in it:
                          rows.append({"symbol":sym,"statement":stmt,"period":period,"metric":k,"value":it.get(k)})
              return rows

          def main():
              wl=set(read_symbols(os.getenv("WATCHLIST_STOCKS"))+read_symbols(os.getenv("WATCHLIST_ETF")))
              out="data/processed/financials_timeseries.csv"
              rows=[]
              for sym in sorted(wl):
                  for stmt in ["ic","bs","cf"]:
                      try: rows.extend(pull_statement(sym, stmt))
                      except Exception: pass
              pd.DataFrame(rows).to_csv(out, index=False)
              print("wrote", out, "rows", len(rows))

          if __name__=="__main__": main()
          PY


      # -------------------- PEERS (für Wettbewerbsvergleich) ------------------
      - name: Create scripts/fetch_peers.py
        run: |
          cat > scripts/fetch_peers.py <<'PY'
          import os, time, pandas as pd, requests
          BASE="https://finnhub.io/api/v1"
          TOKEN=os.getenv("FINNHUB_TOKEN") or os.getenv("FINNHUB_API_KEY")

          def q(url, params):
              params=dict(params or {}); 
              if TOKEN: params["token"]=TOKEN
              for i in range(3):
                  try:
                      r=requests.get(url, params=params, timeout=20)
                      if r.status_code==200: return r.json()
                  except Exception: pass
                  time.sleep(1+2*i)
              return None

          def read_symbols(p):
              if not p or not os.path.exists(p): return []
              df=pd.read_csv(p); col="symbol" if "symbol" in df.columns else df.columns[0]
              return [str(x).strip().upper() for x in df[col].dropna().tolist()]

          def main():
              wl=set(read_symbols(os.getenv("WATCHLIST_STOCKS"))+read_symbols(os.getenv("WATCHLIST_ETF")))
              out="data/processed/peers.csv"; rows=[]
              for sym in sorted(wl):
                  arr = q(BASE+"/stock/peers", {"symbol":sym}) or []
                  for p in arr or []:
                      rows.append({"symbol":sym,"peer":str(p).upper()})
              pd.DataFrame(rows).to_csv(out, index=False)
              print("wrote", out, "rows", len(rows))

          if __name__=="__main__": main()
          PY

      - name: Fetch peers (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_peers.py
          head -n 10 data/processed/peers.csv || true


      # -------------------- COT 20y Pulls ---------------------------
      - name: Pull COT 20y – Disaggregated (F+O Combined)
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          echo "== Socrata Probe (Disaggregated) =="
          for i in 1 2 3; do
            if curl -fsS -G "${CFTC_API_BASE}/${COT_DISAGG_DATASET_ID}.json" \
                 --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
                 --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
                 --data-urlencode '$limit=1' \
                 -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq .; then break; fi
            echo "probe failed (try $i) – sleep 10s"; sleep 10
          done
          echo "== Pull 20y Disaggregated =="
          if ! python -u scripts/fetch_cot_20y.py \
                --dataset "${COT_DISAGG_DATASET_ID}" \
                --out "data/processed/cot_20y_disagg.csv.gz"; then
            echo "WARN: disagg pull failed (server 503?) – keep going"
          fi
          ls -lh data/processed/cot_20y_disagg.csv.gz || true
          zcat data/processed/cot_20y_disagg.csv.gz | head -n 2 || true

      - name: Pull COT 20y – TFF (F+O Combined)
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          echo "== Socrata Probe (TFF) =="
          for i in 1 2 3; do
            if curl -fsS -G "${CFTC_API_BASE}/${COT_TFF_DATASET_ID}.json" \
                 --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
                 --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
                 --data-urlencode '$limit=1' \
                 -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq .; then break; fi
            echo "probe failed (try $i) – sleep 10s"; sleep 10
          done
          echo "== Pull 20y TFF =="
          if ! python -u scripts/fetch_cot_20y.py \
                --dataset "${COT_TFF_DATASET_ID}" \
                --out "data/processed/cot_20y_tff.csv.gz"; then
            echo "WARN: tff pull failed (server 503?) – keep going"
          fi
          ls -lh data/processed/cot_20y_tff.csv.gz || true
          zcat data/processed/cot_20y_tff.csv.gz | head -n 2 || true


      - name: Build COT market coverage report
        run: |
          python scripts/build_cot_coverage.py
          echo "== COT Coverage =="
          head -n 40 data/reports/cot_markets_coverage.csv || true
          echo "== fehlende Watchlist-Einträge =="
          cat data/reports/cot_markets_missing.txt || true
          echo "== alle CFTC-Marktnamen =="
          head -n 40 data/reports/cot_market_names_all.txt || true

      - name: Fetch CFTC Energy Disagg (Historical Compressed)
        run: |
          python scripts/fetch_cftc_energy_disagg.py
          zcat data/processed/cot_disagg_energy_raw.csv.gz | head -n 20 || true

      - name: Build COT Energy Coverage
        run: |
          python scripts/build_cot_energy_coverage.py
          head -n 40 data/reports/cot_energy_coverage.csv || true

      - name: Merge COT Energy into 20y Disagg
        run: |
          set -euo pipefail
          python scripts/merge_cot_energy_into_20y.py
          echo "== Merged COT 20y Disagg (Socrata + Energy TXT) =="
          ls -lh data/processed/cot_20y_disagg_merged.csv.gz || true
          zcat data/processed/cot_20y_disagg_merged.csv.gz | head -n 20 || true



      # -------------------- HV (historische Volatilität) ---------------------
      - name: Build HV summary (Stooq → hv20/hv60, mit yfinance-Fallback)
        run: |
          python scripts/build_hv_summary.py \
            --watchlist "${WATCHLIST_STOCKS}" \
            --days 252 \
            --out data/processed/hv_summary.csv.gz \
            --max_workers 8 \
            --yf-fallback
          test -s data/processed/hv_summary.csv.gz || (echo "hv_summary fehlt!" && exit 1)
          zcat data/processed/hv_summary.csv.gz | head -n 10 || true

      # ===================== RiskIndex Inputs (Core) =========================
      - name: Fetch market core quotes (VIX/DXY/HYG/LQD/…)
        run: |
          python scripts/fetch_market_core.py
          zcat data/processed/market_core.csv.gz | head -n 5 || true
          python - <<'PY'
          import pandas as pd
          try:
              df = pd.read_csv("data/processed/market_core.csv.gz", compression="gzip")
              nn = df.notna().sum().to_dict()
              print("market_core nonnull:", nn)
              need = ["SPY","HYG","LQD","XLF"]
              missing = [c for c in need if nn.get(c,0)==0]
              if missing:
                  raise SystemExit("Fehlende Market-Core Spalten ohne Daten: " + ", ".join(missing))
          except Exception as e:
              print("WARN:", e)
          PY

      - name: Fetch FRED core (macro & OAS)
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          python scripts/fetch_fred_core.py
          zcat data/processed/fred_core.csv.gz | head -n 5 || true
          zcat data/processed/fred_oas.csv.gz  | head -n 5 || true

      # -------------------- CDS Proxy aus OAS/ETF ----------------------------
      - name: Build CDS proxy (v2) from OAS/ETF
        run: |
          python scripts/build_cds_proxy_v2.py \
            --watchlist "${WATCHLIST_STOCKS}" \
            --fred-oas data/processed/fred_oas.csv \
            --fundamentals data/processed/fundamentals_core.csv \
            --hv data/processed/hv_summary.csv.gz \
            --eu-hy-alpha 1.0 --eu-hy-premium 0.20 \
            --hv-anchor 0.25 --hv-min 0.85 --hv-max 1.15
          test -s data/processed/cds_proxy.csv || (echo "cds_proxy fehlt!" && exit 1)
          head -n 5 data/processed/cds_proxy.csv || true

      - name: (optional) Assemble prices parquet
        run: |
          if [ -f "scripts/build_prices_parquet.py" ]; then
            python scripts/build_prices_parquet.py || true
          fi

      # -------------------- Corporate Actions (reine Rohdaten) ---------------
      - name: Fetch Dividends & Splits (yfinance, Rohdaten)
        run: |
          python - <<'PY'
          import os, pandas as pd, yfinance as yf
          os.makedirs("data/processed", exist_ok=True)
          def read_list(p):
              if not p or not os.path.exists(p): return []
              df = pd.read_csv(p)
              col = "symbol" if "symbol" in df.columns else df.columns[0]
              return [str(s).strip().upper() for s in df[col].dropna().tolist()]
          wl = sorted(set(read_list(os.getenv("WATCHLIST_STOCKS")) + read_list(os.getenv("WATCHLIST_ETF"))))
          div_rows, split_rows = [], []
          for sym in wl:
              try:
                  t = yf.Ticker(sym)
                  div = t.dividends
                  if div is not None and len(div):
                      for dt,val in div.sort_index().items():
                          div_rows.append({"symbol":sym,"date":str(dt.date()),"dividend":float(val)})
                  sp = t.splits
                  if sp is not None and len(sp):
                      for dt,val in sp.sort_index().items():
                          split_rows.append({"symbol":sym,"date":str(dt.date()),"split_ratio":float(val)})
              except Exception:
                  pass
          pd.DataFrame(div_rows).to_csv("data/processed/dividends.csv", index=False)
          pd.DataFrame(split_rows).to_csv("data/processed/splits.csv", index=False)
          print("dividends rows:", len(div_rows), "splits rows:", len(split_rows))
          PY
          test -f data/processed/dividends.csv && head -n 10 data/processed/dividends.csv || true
          test -f data/processed/splits.csv && head -n 10 data/processed/splits.csv || true

      # -------------------- Equity MASTER (RAW Merge, keine Scores) ----------
      - name: Build Equity MASTER (raw merge)
        run: |
          cat > scripts/build_equity_master.py <<'PY'
          #!/usr/bin/env python3
          import os, json, pandas as pd
          PROC, DOCS = "data/processed", "docs"

          def rd_csv(*c):
              for p in c:
                  if p and os.path.exists(p):
                      try:
                          return pd.read_csv(p, compression="infer")
                      except Exception:
                          pass
              return None

          def rd_json(*c):
              for p in c:
                  if os.path.exists(p):
                      try:
                          j = json.load(open(p, "r", encoding="utf-8"))
                          if isinstance(j, list):
                              return pd.DataFrame(j)
                          if isinstance(j, dict):
                              return pd.DataFrame(j.get("items", [])) if "items" in j else pd.DataFrame([j])
                      except Exception:
                          pass
              return None

          def norm(df):
              if df is not None and "symbol" in df.columns:
                  df["symbol"] = df["symbol"].astype(str).str.upper().str.strip()
              return df

          def left(m, df, cols=None, ren=None):
              if df is None or df.empty:
                  return m
              if ren:
                  df = df.rename(columns=ren)
              if cols:
                  df = df[[c for c in cols if c in df.columns]].copy()
              return m.merge(df, how="left", on="symbol")

          def stance_from_dir(row):
              d = row.get("dir")
              if pd.isna(d):
                  return "neutral"
              try:
                  d = int(d)
              except Exception:
                  return "neutral"
              return "bullish" if d > 0 else ("bearish" if d < 0 else "neutral")

          def build(out):
              sets = []

              # ---- Options-Signale & OI-Quellen --------------------------------
              dirsig = norm(rd_csv(f"{PROC}/direction_signal.csv", f"{PROC}/direction_signal.csv.gz"))
              if dirsig is not None: sets.append(dirsig[["symbol"]])

              optsig = norm(rd_csv(f"{PROC}/options_signals.csv", f"{PROC}/options_signals.csv.gz"))
              if optsig is not None: sets.append(optsig[["symbol"]])

              bystrk = norm(rd_csv(f"{PROC}/options_oi_by_strike.csv", f"{PROC}/options_oi_by_strike.csv.gz"))
              if bystrk is not None: sets.append(bystrk[["symbol"]])

              optsum = norm(rd_csv(f"{PROC}/options_oi_summary.csv", f"{PROC}/options_oi_summary.csv.gz"))
              if optsum is not None: sets.append(optsum[["symbol"]])

              opttot = norm(rd_csv(f"{PROC}/options_oi_totals.csv", f"{PROC}/options_oi_totals.csv.gz"))
              if opttot is not None: sets.append(opttot[["symbol"]])

              # ---- Rest wie bisher ---------------------------------------------
              hv   = norm(rd_csv(f"{PROC}/hv_summary.csv", f"{PROC}/hv_summary.csv.gz"))
              if hv is not None: sets.append(hv[["symbol"]])

              fnda = norm(rd_csv(f"{PROC}/fundamentals_core.csv", f"{PROC}/fundamentals_core.csv.gz"))
              if fnda is not None: sets.append(fnda[["symbol"]])

              cds  = norm(rd_csv(f"{PROC}/cds_proxy.csv", f"{PROC}/cds_proxy.csv.gz",
                                 f"{PROC}/cds_proxy_v3.csv", f"{PROC}/cds_proxy_v3.csv.gz"))
              if cds is not None: sets.append(cds[["symbol"]])

              rev   = norm(rd_csv(f"{PROC}/revisions.csv", f"{PROC}/revisions.csv.gz"))
              if rev is not None: sets.append(rev[["symbol"]])

              earn  = norm(rd_json(f"{DOCS}/earnings_next.json", f"{DOCS}/earnings_next.json.gz"))
              if earn is not None: sets.append(earn[["symbol"]])

              si    = norm(rd_csv(f"{PROC}/short_interest.csv", f"{PROC}/short_interest.csv.gz"))
              if si is not None: sets.append(si[["symbol"]])

              peers = norm(rd_csv(f"{PROC}/peers.csv", f"{PROC}/peers.csv.gz"))
              if peers is not None: sets.append(peers[["symbol"]])

              if not sets:
                  raise SystemExit("keine Inputs gefunden")

              uni = pd.concat(sets, ignore_index=True).drop_duplicates()
              master = uni.drop_duplicates("symbol").copy()

              # ---------- direction_signal (dir/strength + Fokus-Strikes) -------
              if dirsig is not None:
                  ren = {"focus_strike_7":"fs7","focus_strike_30":"fs30","focus_strike_60":"fs60",
                         "direction":"dir"}
                  cols = [c for c in [
                      "symbol","dir","strength","next_expiry","nearest_dte",
                      "focus_strike_7","focus_strike_30","focus_strike_60","direction"
                  ] if c in dirsig.columns]
                  master = left(master, dirsig, cols=cols, ren=ren)

              # ---------- options_signals (Fallback für dir/strength/strike) ----
              if optsig is not None:
                  ren = {"direction":"dir"} if "direction" in optsig.columns else None
                  cols = [c for c in ["symbol","dir","strength","expiry","strike","direction"] if c in optsig.columns]
                  master = left(master, optsig, cols=cols, ren=ren)

              # ---------- options_oi_by_strike (Focus-Strike + DTE) -------------
              if bystrk is not None:
                  bs = bystrk.copy()
                  if "focus_strike" in bs.columns:
                      bs = bs.rename(columns={"focus_strike":"focus_strike_general"})
                  cols = [c for c in ["symbol","focus_strike_general","expiry","dte"] if c in bs.columns]
                  master = left(master, bs, cols=cols)

              # ---------- options_oi_summary (dominanter Verfall pro Symbol) ----
              if optsum is not None and not optsum.empty:
                  s = optsum.copy()
                  if "call_oi" in s.columns:
                      s["call_oi"] = pd.to_numeric(s["call_oi"], errors="coerce")
                  if "put_oi" in s.columns:
                      s["put_oi"] = pd.to_numeric(s["put_oi"], errors="coerce")
                  if "call_oi" in s.columns and "put_oi" in s.columns:
                      s["__tot_oi"] = s["call_oi"].fillna(0.0) + s["put_oi"].fillna(0.0)
                      s = s.sort_values(["symbol","__tot_oi"], ascending=[True,False]).drop_duplicates("symbol")
                  else:
                      s = s.drop_duplicates("symbol")

                  ren = {
                      "expiry":           "opt_expiry",
                      "spot":             "opt_spot",
                      "call_oi":          "opt_call_oi",
                      "put_oi":           "opt_put_oi",
                      "put_call_ratio":   "opt_put_call_ratio",
                      "call_iv_w":        "opt_call_iv_w",
                      "put_iv_w":         "opt_put_iv_w",
                      "call_top_strikes": "opt_call_top_strikes",
                      "put_top_strikes":  "opt_put_top_strikes",
                  }
                  cols = ["symbol"] + [c for c in ren.keys() if c in s.columns]
                  master = left(master, s, cols=cols, ren=ren)

              # ---------- options_oi_totals (Summe über alle Verfälle) ----------
              if opttot is not None and not opttot.empty:
                  t = opttot.copy()
                  cl = {c.lower(): c for c in t.columns}

                  call_candidates = ["call_oi","total_call_oi","call_oi_all","calloi","call_oi_total"]
                  put_candidates  = ["put_oi","total_put_oi","put_oi_all","putoi","put_oi_total"]

                  call_col = next((cl[x] for x in call_candidates if x in cl), None)
                  put_col  = next((cl[x] for x in put_candidates  if x in cl), None)

                  if call_col and put_col:
                      t[call_col] = pd.to_numeric(t[call_col], errors="coerce")
                      t[put_col]  = pd.to_numeric(t[put_col],  errors="coerce")

                      if "symbol" in t.columns and t["symbol"].nunique() < len(t):
                          agg = t.groupby("symbol", as_index=False).agg({
                              call_col: "sum",
                              put_col:  "sum",
                          })
                      else:
                          agg = t[["symbol", call_col, put_col]].drop_duplicates("symbol")

                      agg = agg.rename(columns={
                          call_col: "tot_call_oi_all",
                          put_col:  "tot_put_oi_all",
                      })
                      if "tot_call_oi_all" in agg.columns and "tot_put_oi_all" in agg.columns:
                          agg["tot_put_call_ratio_all"] = agg["tot_put_oi_all"] / agg["tot_call_oi_all"].replace(0, pd.NA)

                      master = left(master, agg, cols=agg.columns.tolist())
                  # wenn keine passenden Spalten gefunden wurden: einfach überspringen

              # ---------- HV ----------------------------------------------------
              if hv is not None:
                  hv = hv.rename(columns={c:c.lower() for c in hv.columns})
                  cols = [c for c in ["symbol","hv10","hv20","hv30","hv60"] if c in hv.columns]
                  master = left(master, hv, cols=cols)

              # ---------- Fundamentals -----------------------------------------
              if fnda is not None:
                  keep = [c for c in [
                      "symbol","name","sector","industry","currency",
                      "marketcap","sharesoutstanding",
                      "pe","pb","ps","ev_ebitda","beta"
                  ] if c in fnda.columns]
                  master = left(master, fnda, cols=keep)

              # ---------- Earnings next ----------------------------------------
              if earn is not None:
                  ren = {"next_date":"earnings_next"} if "next_date" in earn.columns else None
                  cols = [c for c in ["symbol","earnings_next","next_date"] if c in earn.columns]
                  master = left(master, earn, cols=cols, ren=ren)

              # ---------- CDS proxy --------------------------------------------
              if cds is not None:
                  ren = {"proxy_spread":"cds_proxy"} if "proxy_spread" in cds.columns else None
                  cols = [c for c in ["symbol","cds_proxy","proxy_spread"] if c in cds.columns]
                  master = left(master, cds, cols=cols, ren=ren)

              # ---------- Revisions --------------------------------------------
              if rev is not None:
                  keep = [c for c in [
                      "symbol","eps_rev_3m","rev_rev_3m","eps_surprise","rev_surprise"
                  ] if c in rev.columns]
                  master = left(master, rev, cols=keep)

              # ---------- Short Interest ---------------------------------------
              if si is not None:
                  keep = [c for c in [
                      "symbol","si_date","si_shares","float_shares",
                      "si_pct_float","borrow_rate","borrow_avail"
                  ] if c in si.columns]
                  master = left(master, si, cols=keep)

              # ---------- Peers count ------------------------------------------
              if peers is not None and "peer" in peers.columns:
                  pc = (peers.groupby("symbol")["peer"]
                             .nunique()
                             .reset_index()
                             .rename(columns={"peer":"peers_count"}))
                  master = left(master, pc, cols=["symbol","peers_count"])

              # ---------- Stance aus dir ---------------------------------------
              master["stance"] = master.apply(stance_from_dir, axis=1)

              # ---------- Spalten sortieren ------------------------------------
              preferred = [
                  "symbol","name","sector","industry",
                  "stance","dir","strength",
                  "next_expiry","nearest_dte",
                  "fs7","fs30","fs60","focus_strike_general",
                  "opt_expiry","opt_spot",
                  "opt_call_oi","opt_put_oi","opt_put_call_ratio",
                  "opt_call_iv_w","opt_put_iv_w",
                  "opt_call_top_strikes","opt_put_top_strikes",
                  "tot_call_oi_all","tot_put_oi_all","tot_put_call_ratio_all",
                  "earnings_next",
                  "hv10","hv20","hv30","hv60",
                  "cds_proxy",
                  "marketcap","pe","pb","ps","ev_ebitda","beta","currency",
                  "si_date","si_shares","float_shares","si_pct_float",
                  "borrow_rate","borrow_avail",
                  "peers_count"
              ]
              cols = [c for c in preferred if c in master.columns] + \
                     [c for c in master.columns if c not in preferred]
              master = master[cols]

              os.makedirs(os.path.dirname(out), exist_ok=True)
              master.to_csv(out, index=False)
              print("✅ wrote", out, "rows", len(master), "cols", len(master.columns))

          if __name__=="__main__":
              build_out = os.getenv("OUT","data/processed/equity_master.csv")
              build(build_out)
          PY
          python scripts/build_equity_master.py
          test -s data/processed/equity_master.csv || (echo "equity_master leer!" && exit 1)
          head -n 20 data/processed/equity_master.csv || true



      # -------------------- RiskIndex snapshot & timeseries -------------------
      - name: Build RiskIndex snapshot & timeseries
        run: |
          python scripts/build_riskindex.py
          test -s data/processed/riskindex_snapshot.json || (echo "riskindex_snapshot fehlt!" && exit 1)
          test -s data/processed/riskindex_timeseries.csv || echo "riskindex_timeseries leer/fehlend (ok)"
          head -n 5 data/processed/riskindex_timeseries.csv || true
          cat data/processed/riskindex_snapshot.json || true

      # ===================== ALLES KOMPRIMIEREN ===============================
      - name: Compress ALL processed & docs to .gz
        shell: bash
        run: |
          set -e
          shopt -s nullglob
          for f in data/processed/*.{csv,json,ndjson,parquet}; do [ -f "$f" ] && gzip -f -9 "$f" || true; done
          for f in docs/*.{json,csv}; do [ -f "$f" ] && gzip -f -9 "$f" || true; done
          echo "== processed after gzip =="; ls -lh data/processed || true
          echo "== docs after gzip =="; ls -lh docs || true

      # -------------------- DAILY SNAPSHOTS ARCHIVIEREN ----------------------
      - name: Archive daily snapshots to data/history
        shell: bash
        run: |
          set -e
          ts=$(date -u +%F)
          dest="data/history/$ts"
          mkdir -p "$dest"

          copy() {
            local f="$1"
            if [ -f "$f" ]; then
              cp -f "$f" "$dest/"
            else
              echo "skip missing $f"
            fi
          }

          # Wichtige Dateien konservieren
          copy data/processed/equity_master.csv.gz
          copy data/processed/options_signals.csv.gz
          copy data/processed/direction_signal.csv.gz
          copy data/processed/options_oi_by_expiry.csv.gz
          copy data/processed/options_oi_by_strike.csv.gz
          copy data/processed/options_oi_totals.csv.gz
          copy data/processed/short_interest.csv.gz
          copy data/processed/insider_tx.csv.gz
          copy data/processed/fund_ownership.csv.gz
          copy data/processed/hv_summary.csv.gz
          copy data/processed/fred_core.csv.gz
          copy data/processed/fred_oas.csv.gz
          copy data/processed/market_core.csv.gz
          copy data/processed/cds_proxy.csv.gz
          copy data/processed/revisions.csv.gz
          copy data/processed/dividends.csv.gz
          copy data/processed/splits.csv.gz

          echo "archived to $dest:"
          ls -lh "$dest" || true


      # Wenn kein R2 konfiguriert ist → processed als Artifact sichern
      - name: Upload processed (artifact when no R2)
        if: ${{ !(env.CF_R2_BUCKET != '' && env.CF_R2_ENDPOINT != '' && env.CF_R2_ACCESS_KEY_ID != '' && env.CF_R2_SECRET_ACCESS_KEY != '') }}
        uses: actions/upload-artifact@v4
        with:
          name: processed-gz
          path: |
            data/processed/**/*.gz
            data/history/**/*.gz

      # ===================== R2 UPLOADS (ein Bucket) ==========================
      - name: Sync .gz (processed & docs) to R2
        if: ${{ env.CF_R2_BUCKET != '' && env.CF_R2_ENDPOINT != '' && env.CF_R2_ACCESS_KEY_ID != '' && env.CF_R2_SECRET_ACCESS_KEY != '' }}
        env:
          CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
          CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}
          CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
          CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          cat > rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${CF_R2_ACCESS_KEY_ID}
          secret_access_key = ${CF_R2_SECRET_ACCESS_KEY}
          endpoint = ${CF_R2_ENDPOINT}
          acl = public-read
          force_path_style = true
          no_check_bucket = true
          EOF
          rclone sync data/processed "r2:${CF_R2_BUCKET}/data/processed" --config rclone.conf --s3-no-check-bucket -v --include "*.gz" --exclude "*"
          rclone sync data/history   "r2:${CF_R2_BUCKET}/data/history"   --config rclone.conf --s3-no-check-bucket -v --include "**/*.gz" --exclude "*"
          rclone sync docs           "r2:${CF_R2_BUCKET}/docs"           --config rclone.conf --s3-no-check-bucket -v --include "*.gz" --exclude "*"

      # ===================== REPO AUFRÄUMEN & COMMIT =========================
      - name: Purge heavy local files before commit
        shell: bash
        run: |
          set -e
          find data/processed -type f ! -name ".gitkeep" -delete || true

      - name: Build datasets manifest (docs/datasets.json)
        env:
          PUBLIC_BASE: ${{ vars.CF_R2_PUBLIC_BASE }}
        run: |
          mkdir -p docs
          python - <<'PY'
          import json, os, pathlib
          base = os.getenv("PUBLIC_BASE", "https://pub-CHANGE-ME.r2.dev")
          manifest = {
            "base": base,
            "datasets": {
              "cot_20y_disagg":        "/data/processed/cot_20y_disagg.csv.gz",
              "cot_20y_tff":           "/data/processed/cot_20y_tff.csv.gz",
              "options_by_expiry":     "/data/processed/options_oi_by_expiry.csv.gz",
              "options_totals":        "/data/processed/options_oi_totals.csv.gz",
              "options_by_strike":     "/data/processed/options_oi_by_strike.csv.gz",
              "options_strike_max":    "/data/processed/options_oi_strike_max.csv.gz",
              "options_signals":       "/data/processed/options_signals.csv.gz",
              "direction_signal":      "/data/processed/direction_signal.csv.gz",
              "short_interest":        "/data/processed/short_interest.csv.gz",
              "insider_tx":            "/data/processed/insider_tx.csv.gz",
              "fund_ownership":        "/data/processed/fund_ownership.csv.gz",
              "company_news":          "/data/processed/company_news.ndjson.gz",
              "peers":                 "/data/processed/peers.csv.gz",
              "dividends":             "/data/processed/dividends.csv.gz",
              "splits":                "/data/processed/splits.csv.gz",
              "fred_core":             "/data/processed/fred_core.csv.gz",
              "fred_oas":              "/data/processed/fred_oas.csv.gz",
              "market_core":           "/data/processed/market_core.csv.gz",
              "hv_summary":            "/data/processed/hv_summary.csv.gz",
              "cds_proxy":             "/data/processed/cds_proxy.csv.gz",
              "equity_master":         "/data/processed/equity_master.csv.gz",
              "riskindex_snapshot":    "/data/processed/riskindex_snapshot.json.gz",
              "riskindex_timeseries":  "/data/processed/riskindex_timeseries.csv.gz"
            }
          }
          pathlib.Path("docs").mkdir(parents=True, exist_ok=True)
          open("docs/datasets.json","w",encoding="utf-8").write(json.dumps(manifest,indent=2,ensure_ascii=False))
          print("datasets.json base =", base)
          PY

      - name: Commit updated cache & reports (no big files)
        shell: bash
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/ docs/ \
            ':!data/processed/*.csv' ':!data/processed/*.csv.gz' \
            ':!data/processed/*.parquet' ':!data/processed/*.zip' ':!data/processed/*.json.gz' || true
          git commit -m "Nightly cache update (No LLM) + RAW data expansion (SI/Insider/Fund/Financials/Peers/News/Div&Splits) + Equity MASTER + RiskIndex/CDS/HV/COT/OAS + History" || echo "Nothing to commit"
          git push

      - name: Upload reports & docs (artifact)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: reports-and-docs
          path: |
            data/reports/**
            docs/**

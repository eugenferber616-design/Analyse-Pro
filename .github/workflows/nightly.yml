name: Nightly Data Pull and Cache (No LLM)

on:
  schedule:
    - cron: "30 0 * * *"   # täglich 00:30 UTC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      WATCHLIST_STOCKS: watchlists/mylist.txt
      WATCHLIST_ETF:    watchlists/etf_sample.txt
      FORKAST_SUBDIR:   "prompter"   # unbenutzt, kann bleiben

      # Kurse/History
      PRICES_DAYS:      "750"
      FINNHUB_SLEEP_MS: "1300"

      # Earnings / Optionen / HV
      WINDOW_DAYS:          "30"
      OPTIONS_MAX_EXPIRIES: "all"
      OPTIONS_TOPK:         "5"
      RISK_FREE_RATE:       "0.045"
      HV_WIN_SHORT:         "20"
      HV_WIN_LONG:          "60"
      HV_WIN_10:            "10"
      HV_WIN_30:            "30"

      ENABLE_FUTURES: "false"

      # COT – Socrata Basis
      CFTC_API_BASE:         "https://publicreporting.cftc.gov/resource"
      COT_DISAGG_DATASET_ID: "kh3c-gbw2"
      COT_TFF_DATASET_ID:    "yw9f-hn96"
      COT_YEARS:             "20"
      COT_MARKETS_MODE:      "SMART"        # "ALL" | "FILE" | "LIST" | "SMART"
      COT_MARKETS_FILE:      watchlists/cot_markets.txt
      SOC_TIMEOUT:           "120"
      SOC_RETRIES:           "8"
      SOC_BACKOFF:           "2.0"
      SOC_LIMIT:             "20000"

      # FRED
      FRED_START: "2003-01-01"

      # Template-Analyse (neu)
      EQ_OUTDIR:  data/processed/eq_template
      EQ_SITE:    site/eq
      PUBLIC_BASE: ${{ vars.CF_R2_PUBLIC_BASE }}

      # lokale Module auffindbar
      PYTHONPATH: "."

      # Secrets in env spiegeln → if: kann env.* prüfen
      CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
      CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
      CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
      CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Fallbacks/Extras
          pip install --upgrade yfinance pandas_datareader
          # parquet-Engines optional
          pip install --upgrade pyarrow fastparquet matplotlib
          # für Template-Analyse
          pip install --upgrade pandas requests python-dateutil

      - name: Init cache DB
        run: |
          mkdir -p data/cache data/earnings data/macro/fred data/macro/ecb data/market/stooq data/market/core data/processed data/reports docs watchlists scripts
          python - << 'PY'
          import os, sqlite3
          db_path = os.path.join('data','cache','cache.db')
          os.makedirs(os.path.dirname(db_path), exist_ok=True)
          con = sqlite3.connect(db_path)
          cur = con.cursor()
          cur.execute("""
          CREATE TABLE IF NOT EXISTS kv (
            k  TEXT PRIMARY KEY,
            v  TEXT NOT NULL,
            ts INTEGER DEFAULT (strftime('%s','now'))
          )""")
          cur.execute("CREATE INDEX IF NOT EXISTS ix_kv_ts ON kv(ts)")
          con.commit(); con.close()
          print("cache DB ready at", db_path)
          PY

      - name: Prepare env and watchlists
        id: prep
        shell: bash
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          set -e
          TOKEN="${FINNHUB_TOKEN:-$FINNHUB_API_KEY}"
          if [ -z "$TOKEN" ]; then
            echo "❌ FINNHUB_TOKEN/FINNHUB_API_KEY fehlt."
            exit 1
          fi
          echo "token_present=true" >> "$GITHUB_OUTPUT"

          if [ ! -f "$WATCHLIST_STOCKS" ]; then
            printf "symbol\nAAPL\nMSFT\nNVDA\nSPY\n" > watchlists/mylist.csv
            echo "WATCHLIST_STOCKS=watchlists/mylist.csv" >> "$GITHUB_ENV"
          fi
          echo "== Aktien Watchlist =="; head -n 10 "${WATCHLIST_STOCKS}" || true

          if [ ! -f "$WATCHLIST_ETF" ]; then
            printf "SPY\nQQQ\nIWM\nGLD\nHYG\nXLF\nXLK\nXLE\n" > watchlists/etf_sample.txt
          fi
          echo "== ETF Sample =="; head -n 10 "${WATCHLIST_ETF}" || true

      # ---------------- Earnings / Fundamentals / Optionen -------------------

      - name: Pull earnings CALENDAR (optional)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          mkdir -p data/earnings docs
          python scripts/fetch_earnings.py --watchlist "${WATCHLIST_STOCKS}" --window-days "${WINDOW_DAYS}"
          test -f docs/earnings_next.json && head -n 50 docs/earnings_next.json || true

      - name: Pull earnings RESULTS (historisch, robust)
        env:
          FINNHUB_TOKEN:        ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY:      ${{ secrets.FINNHUB_API_KEY }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          SEC_USER_AGENT:       ${{ secrets.SEC_USER_AGENT }}
          SIMFIN_API_KEY:       ${{ secrets.SIMFIN_API_KEY }}
          FINNHUB_SLEEP_MS:     ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          python scripts/fetch_earnings_results.py --watchlist "${WATCHLIST_STOCKS}" --limit 40
          test -f data/processed/earnings_results.csv && head -n 20 data/processed/earnings_results.csv || true

      - name: Pull ETF basics (Finnhub)
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_etf_basics.py --watchlist "${WATCHLIST_ETF}" \
            --out data/processed/etf_basics.csv \
            --errors data/reports/etf_errors.json || true
          head -n 30 data/processed/etf_basics.csv || true
          test -f data/reports/etf_errors.json && cat data/reports/etf_errors.json || true

      - name: Pull FUNDAMENTALS (profile2 + metrics, mit Fallbacks)
        env:
          FINNHUB_API_KEY:  ${{ secrets.FINNHUB_API_KEY }}
          FINNHUB_TOKEN:    ${{ secrets.FINNHUB_TOKEN }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          SIMFIN_API_KEY:   ${{ secrets.SIMFIN_API_KEY }}
        run: |
          python scripts/fetch_fundamentals.py \
            --watchlist "${WATCHLIST_STOCKS}" \
            --sleep_ms 200 || true
          head -n 20 data/processed/fundamentals_core.csv || true

      - name: Fetch Options OI + IV/HV summary (Stocks + ETFs)
        shell: bash
        env:
          OPTIONS_MAX_EXPIRIES: ${{ env.OPTIONS_MAX_EXPIRIES }}
          OPTIONS_TOPK:         ${{ env.OPTIONS_TOPK }}
          HV_WIN_SHORT:         ${{ env.HV_WIN_SHORT }}
          HV_WIN_LONG:          ${{ env.HV_WIN_LONG }}
          HV_WIN_10:            ${{ env.HV_WIN_10 }}
          HV_WIN_30:            ${{ env.HV_WIN_30 }}
        run: |
          set -e
          tmp=/tmp/options_watchlist.txt
          {
            if [ -f "${WATCHLIST_STOCKS}" ]; then
              tail -n +2 "${WATCHLIST_STOCKS}" | cut -d',' -f1 | tr -d '\r'
            fi
            if [ -f "${WATCHLIST_ETF}" ]; then
              if head -n1 "${WATCHLIST_ETF}" | grep -qi '^symbol'; then
                tail -n +2 "${WATCHLIST_ETF}" | tr -d '\r'
              else
                cat "${WATCHLIST_ETF}" | tr -d '\r'
              fi
            fi
          } | awk 'NF>0' | sort -u > "$tmp"

          python scripts/fetch_options_oi.py --watchlist "$tmp"
          test -s data/processed/options_oi_summary.csv     || (echo "options_oi_summary leer!" && exit 1)
          test -s data/processed/options_oi_by_expiry.csv   || (echo "options_oi_by_expiry leer!" && exit 1)
          test -s data/processed/options_oi_totals.csv      || (echo "options_oi_totals leer!" && exit 1)
          head -n 40 data/processed/options_oi_summary.csv  || true
          head -n 40 data/processed/options_oi_by_expiry.csv|| true
          head -n 40 data/processed/options_oi_totals.csv   || true

      - name: Build max OI strike per symbol
        run: |
          python scripts/post_build_strike_max.py
          test -s data/processed/options_oi_strike_max.csv || (echo "options_oi_strike_max leer!" && exit 1)
          head -n 20 data/processed/options_oi_strike_max.csv || true

      - name: Build options BY_STRIKE (aus Summary-TopStrikes)
        run: |
          python scripts/build_options_by_strike.py
          test -s data/processed/options_oi_by_strike.csv || (echo "options_oi_by_strike leer!" && exit 1)
          head -n 40 data/processed/options_oi_by_strike.csv || true

      - name: Build options SIGNALS (pro Symbol)
        run: |
          python scripts/build_options_signals.py
          test -s data/processed/options_signals.csv || (echo "options_signals leer!" && exit 1)
          head -n 40 data/processed/options_signals.csv || true

      # -------------------- COT 20y Pulls ---------------------------

      - name: Pull COT 20y – Disaggregated (F+O Combined)
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          echo "== Socrata Probe (Disaggregated) =="; \
          for i in 1 2 3; do
            if curl -fsS -G "${CFTC_API_BASE}/${COT_DISAGG_DATASET_ID}.json" \
                 --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
                 --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
                 --data-urlencode '$limit=1' \
                 -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq .; then break; fi
            echo "probe failed (try $i) – sleep 10s"; sleep 10
          done
          echo "== Pull 20y Disaggregated =="; \
          if ! python -u scripts/fetch_cot_20y.py \
                --dataset "${COT_DISAGG_DATASET_ID}" \
                --out "data/processed/cot_20y_disagg.csv.gz"; then
            echo "WARN: disagg pull failed (server 503?) – keep going"
          fi
          ls -lh data/processed/cot_20y_disagg.csv.gz || true
          zcat data/processed/cot_20y_disagg.csv.gz | head -n 2 || true

      - name: Pull COT 20y – TFF (F+O Combined)
        env:
          CFTC_APP_TOKEN:  ${{ secrets.CFTC_APP_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          echo "== Socrata Probe (TFF) =="; \
          for i in 1 2 3; do
            if curl -fsS -G "${CFTC_API_BASE}/${COT_TFF_DATASET_ID}.json" \
                 --data-urlencode '$select=report_date_as_yyyy_mm_dd' \
                 --data-urlencode '$order=report_date_as_yyyy_mm_dd DESC' \
                 --data-urlencode '$limit=1' \
                 -H "X-App-Token: ${CFTC_APP_TOKEN}" | jq .; then break; fi
            echo "probe failed (try $i) – sleep 10s"; sleep 10
          done
          echo "== Pull 20y TFF =="; \
          if ! python -u scripts/fetch_cot_20y.py \
                --dataset "${COT_TFF_DATASET_ID}" \
                --out "data/processed/cot_20y_tff.csv.gz"; then
            echo "WARN: tff pull failed (server 503?) – keep going"
          fi
          ls -lh data/processed/cot_20y_tff.csv.gz || true
          zcat data/processed/cot_20y_tff.csv.gz | head -n 2 || true

      # -------------------- HV (historische Volatilität) ---------------------

      - name: Build HV summary (Stooq → hv20/hv60, mit yfinance-Fallback)
        run: |
          python scripts/build_hv_summary.py \
            --watchlist "${WATCHLIST_STOCKS}" \
            --days 252 \
            --out data/processed/hv_summary.csv.gz \
            --max_workers 8 \
            --yf-fallback
          test -s data/processed/hv_summary.csv.gz || (echo "hv_summary fehlt!" && exit 1)
          zcat data/processed/hv_summary.csv.gz | head -n 10 || true

      # ===================== RiskIndex Inputs (Core) =========================

      - name: Fetch market core quotes (VIX/DXY/HYG/LQD/…)
        run: |
          python scripts/fetch_market_core.py
          zcat data/processed/market_core.csv.gz | head -n 5 || true
          python - << 'PY'
          import pandas as pd
          try:
              df = pd.read_csv("data/processed/market_core.csv.gz", compression="gzip")
              nn = df.notna().sum().to_dict()
              print("market_core nonnull:", nn)
              need = ["SPY","HYG","LQD","XLF"]
              missing = [c for c in need if nn.get(c,0)==0]
              if missing:
                  raise SystemExit("Fehlende Market-Core Spalten ohne Daten: " + ", ".join(missing))
          except Exception as e:
              print("WARN:", e)
          PY

      - name: Fetch FRED core (macro & OAS)
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          python scripts/fetch_fred_core.py
          zcat data/processed/fred_core.csv.gz | head -n 5 || true
          zcat data/processed/fred_oas.csv.gz  | head -n 5 || true

      # -------------------- CDS Proxy aus OAS/ETF ----------------------------

      - name: Build CDS proxy (v2) from OAS/ETF
        run: |
          python scripts/build_cds_proxy_v2.py \
            --watchlist "${WATCHLIST_STOCKS}" \
            --fred-oas data/processed/fred_oas.csv \
            --fundamentals data/processed/fundamentals_core.csv \
            --hv data/processed/hv_summary.csv.gz \
            --eu-hy-alpha 1.0 --eu-hy-premium 0.20 \
            --hv-anchor 0.25 --hv-min 0.85 --hv-max 1.15
          test -s data/processed/cds_proxy.csv || (echo "cds_proxy fehlt!" && exit 1)
          head -n 5 data/processed/cds_proxy.csv || true

      - name: (optional) Assemble prices parquet
        run: |
          if [ -f "scripts/build_prices_parquet.py" ]; then
            python scripts/build_prices_parquet.py || true
          fi

      - name: Build RiskIndex snapshot & timeseries
        run: |
          python scripts/build_riskindex.py
          test -s data/processed/riskindex_snapshot.json || (echo "riskindex_snapshot fehlt!" && exit 1)
          test -s data/processed/riskindex_timeseries.csv || echo "riskindex_timeseries leer/fehlend (ok)"
          head -n 5 data/processed/riskindex_timeseries.csv || true
          cat data/processed/riskindex_snapshot.json || true

      - name: Analyze equities (Template:HTML + JSON)
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          SEC_USER_AGENT:  ${{ secrets.SEC_USER_AGENT }}
        shell: bash
        run: |
          set -e
          mkdir -p "${EQ_OUTDIR}" "${EQ_SITE}" site
          SYMLIST="$(mktemp)"

          # Spalte 1 (=Symbol) aus Watchlist ziehen (CSV/Plain wird unterstützt)
          if head -n1 "${WATCHLIST_STOCKS}" | grep -qi '^symbol'; then
            tail -n +2 "${WATCHLIST_STOCKS}" | cut -d',' -f1 | tr -d '\r' | awk 'NF>0' > "$SYMLIST"
          else
            sed 's/\r//g' "${WATCHLIST_STOCKS}" | awk 'NF>0' > "$SYMLIST"
          fi

          n=0
          while read -r SYM; do
            [ -z "$SYM" ] && continue
            echo "== Build Template for $SYM =="
            python scripts/analyze_equity_template.py \
              --symbol "$SYM" \
              --out-json "${EQ_OUTDIR}/${SYM}.json" \
              --out-html "${EQ_SITE}/${SYM}.html" \
              --public-base "${PUBLIC_BASE:-https://pub-CHANGE-ME.r2.dev}" || true
            if [ -s "${EQ_OUTDIR}/${SYM}.json" ]; then gzip -f -9 "${EQ_OUTDIR}/${SYM}.json" || true; fi
            n=$((n+1))
          done < "$SYMLIST"
          echo "Anzahl generierter Symbole: $n"

          # Index-Manifest (JSON)
          python - <<'PY'
          import os, json
          d = os.getenv("EQ_OUTDIR","data/processed/eq_template")
          rows = []
          for f in sorted(os.listdir(d)):
              if f.endswith(".json.gz"):
                  s = f[:-8]  # ohne .json.gz
                  rows.append({"symbol": s, "json": f"{s}.json.gz", "html": f"{s}.html"})
          with open(os.path.join(d,"index.json"),"w") as fh:
              json.dump(rows, fh, indent=2)
          PY
          gzip -f -9 "${EQ_OUTDIR}/index.json" || true

          # Einfache HTML-Liste
          {
            echo '<!doctype html><meta charset="utf-8"><title>Equity Templates</title>'
            echo '<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>'
            echo '<style>body{font:14px/1.5 system-ui,Segoe UI,Roboto,Arial;padding:24px;background:#0f1116;color:#e6e6e6}a{color:#7bd4ff;text-decoration:none}a:hover{text-decoration:underline}.card{max-width:900px;margin:0 auto}h1{margin:0 0 8px 0}.muted{color:#98a4ad}ul{columns:3;gap:24px}li{margin:6px 0}</style>'
            echo '<div class="card"><h1>Equity Templates</h1><div class="muted">Ticker aus der Watchlist</div><ul>'
            awk '{print "  <li><a href=\"eq/"$0".html\">" $0 "</a></li>"}' "$SYMLIST"
            echo '</ul></div>'
          } > site/index.html

      # ===================== ALLES KOMPRIMIEREN ====================
      - name: Compress ALL processed & docs to .gz
        shell: bash
        run: |
          set -e
          shopt -s nullglob
          for f in data/processed/*.{csv,json,ndjson,parquet}; do [ -f "$f" ] && gzip -f -9 "$f" || true; done
          for f in docs/*.{json,csv}; do [ -f "$f" ] && gzip -f -9 "$f" || true; done
          echo "== processed after gzip =="; ls -lh data/processed || true
          echo "== docs after gzip =="; ls -lh docs || true

      # ===================== R2 UPLOADS (ein Bucket) ====================
      - name: Sync .gz (processed & docs) to R2
        if: ${{ env.CF_R2_BUCKET != '' && env.CF_R2_ENDPOINT != '' && env.CF_R2_ACCESS_KEY_ID != '' && env.CF_R2_SECRET_ACCESS_KEY != '' }}
        env:
          CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
          CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}
          CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
          CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          cat > rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${CF_R2_ACCESS_KEY_ID}
          secret_access_key = ${CF_R2_SECRET_ACCESS_KEY}
          endpoint = ${CF_R2_ENDPOINT}
          acl = public-read
          force_path_style = true
          no_check_bucket = true
          EOF
          rclone sync data/processed "r2:${CF_R2_BUCKET}/data/processed" --config rclone.conf --s3-no-check-bucket -v --include "*.gz" --exclude "*"
          rclone sync docs           "r2:${CF_R2_BUCKET}/docs"           --config rclone.conf --s3-no-check-bucket -v --include "*.gz" --exclude "*"

      - name: Sync eq site to R2 (debug, full log)
        if: ${{ env.CF_R2_BUCKET != '' && env.CF_R2_ENDPOINT != '' && env.CF_R2_ACCESS_KEY_ID != '' && env.CF_R2_SECRET_ACCESS_KEY != '' }}
        env:
          CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
          CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}
          CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
          CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
        run: |
          set -e
          curl -fsSL https://rclone.org/install.sh | sudo bash
          cat > rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${CF_R2_ACCESS_KEY_ID}
          secret_access_key = ${CF_R2_SECRET_ACCESS_KEY}
          endpoint = ${CF_R2_ENDPOINT}
          force_path_style = true
          no_check_bucket = true
          EOF

          # Quelle leer? -> skip
          if [ ! -d "site/eq" ] || [ -z "$(ls -A site/eq 2>/dev/null)" ]; then
            echo "site/eq ist leer – skip Upload."
            exit 0
          fi

          echo "== Preflight =="
          rclone mkdir "r2:${CF_R2_BUCKET}/site/eq" --config rclone.conf --s3-no-check-bucket || true
          rclone lsd "r2:${CF_R2_BUCKET}"           --config rclone.conf --s3-no-check-bucket || true
          find site/eq -type f -printf "%P\n" | sed 's/^/  /' || true

          echo "== Upload (verbose) =="
          set +e
          rclone sync site/eq "r2:${CF_R2_BUCKET}/site/eq" \
            --config rclone.conf --s3-no-check-bucket \
            -vv --log-file rclone.log --retries 3 --low-level-retries 10 --ignore-errors \
            --s3-chunk-size 64M --s3-upload-concurrency 4
          RC=$?

          # index.html mitkopieren (Fehler hier ignorieren)
          if [ -f site/index.html ]; then
            rclone copy site/index.html "r2:${CF_R2_BUCKET}/site" \
              --config rclone.conf --s3-no-check-bucket -vv --log-file rclone.log || true
          fi

          echo "== rclone LOG (tail) =="
          tail -n 200 rclone.log || true

          # Exit 3 = partial errors -> NICHT fehlschlagen lassen
          if [ "$RC" -eq 0 ] || [ "$RC" -eq 3 ]; then
            exit 0
          else
            exit "$RC"
          fi


      # ===================== REPO AUFRÄUMEN & COMMIT ========================
      - name: Purge heavy local files before commit
        shell: bash
        run: |
          set -e
          find data/processed -type f ! -name ".gitkeep" -delete || true

      - name: Build datasets manifest (docs/datasets.json)
        env:
          PUBLIC_BASE: ${{ vars.CF_R2_PUBLIC_BASE }}
        run: |
          mkdir -p docs
          python - << 'PY'
          import json, os, pathlib
          base = os.getenv("PUBLIC_BASE", "https://pub-CHANGE-ME.r2.dev")
          manifest = {
            "base": base,
            "datasets": {
              "cot_20y_disagg":       "/data/processed/cot_20y_disagg.csv.gz",
              "cot_20y_tff":          "/data/processed/cot_20y_tff.csv.gz",
              "options_by_expiry":    "/data/processed/options_oi_by_expiry.csv.gz",
              "options_totals":       "/data/processed/options_oi_totals.csv.gz",
              "options_by_strike":    "/data/processed/options_oi_by_strike.csv.gz",
              "options_strike_max":   "/data/processed/options_oi_strike_max.csv.gz",
              "options_signals":      "/data/processed/options_signals.csv.gz",
              "direction_signal":     "/data/processed/direction_signal.csv.gz",
              "fred_core":            "/data/processed/fred_core.csv.gz",
              "fred_oas":             "/data/processed/fred_oas.csv.gz",
              "market_core":          "/data/processed/market_core.csv.gz",
              "hv_summary":           "/data/processed/hv_summary.csv.gz",
              "cds_proxy":            "/data/processed/cds_proxy.csv.gz",
              "riskindex_snapshot":   "/data/processed/riskindex_snapshot.json.gz",
              "riskindex_timeseries": "/data/processed/riskindex_timeseries.csv.gz",
              "eq_templates_index":   "/data/processed/eq_template/index.json.gz"
            }
          }
          pathlib.Path("docs").mkdir(parents=True, exist_ok=True)
          open("docs/datasets.json","w",encoding="utf-8").write(json.dumps(manifest,indent=2,ensure_ascii=False))
          print("datasets.json base =", base)
          PY

      - name: Commit updated cache & reports (no big files)
        shell: bash
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/ docs/ \
            ':!data/processed/*.csv' ':!data/processed/*.csv.gz' \
            ':!data/processed/*.parquet' ':!data/processed/*.zip' ':!data/processed/*.json.gz' || true
          git commit -m "Nightly cache update (No LLM) + RiskIndex + CDS/HV/COT/OAS + Eq Templates + deploy" || echo "Nothing to commit"
          git push

      - name: Upload reports & docs (artifact)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: reports-and-docs
          path: |
            data/reports/**
            docs/**

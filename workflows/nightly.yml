name: Nightly Data Pull and Cache (No LLM)

on:
  schedule:
    - cron: "30 0 * * *"   # täglich 00:30 UTC
  workflow_dispatch: {}

permissions:
  contents: write

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      WATCHLIST_STOCKS: watchlists/mylist.txt
      WATCHLIST_ETF:    watchlists/etf_sample.txt
      
      # Kurse/History
      PRICES_DAYS:      "750"
      FINNHUB_SLEEP_MS: "1300"

      # Earnings / Optionen / HV
      WINDOW_DAYS:          "30"
      OPTIONS_MAX_EXPIRIES: "all"
      OPTIONS_TOPK:         "5"
      RISK_FREE_RATE:       "0.045"
      HV_WIN_SHORT:         "20"
      HV_WIN_LONG:          "60"
      HV_WIN_10:            "10"
      HV_WIN_30:            "30"

      ENABLE_FUTURES: "false"

      # COT – Socrata Basis
      CFTC_API_BASE:         "https://publicreporting.cftc.gov/resource"
      COT_DISAGG_DATASET_ID: "kh3c-gbw2"
      COT_TFF_DATASET_ID:    "yw9f-hn96"
      COT_YEARS:             "20"
      COT_MARKETS_MODE:      "SMART"
      COT_MARKETS_FILE:      watchlists/cot_markets.txt
      SOC_TIMEOUT:           "120"
      SOC_RETRIES:           "8"
      SOC_BACKOFF:           "2.0"
      SOC_LIMIT:             "20000"

      # FRED
      FRED_START: "2003-01-01"

      PYTHONPATH: "."

      # Secrets
      CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
      CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
      CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
      CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install jq + rclone
        run: |
          sudo apt-get update
          sudo apt-get install -y jq rclone

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install --upgrade yfinance pandas_datareader requests scipy pyarrow fastparquet matplotlib

      - name: Init cache DB
        shell: bash
        run: |
          mkdir -p data/cache data/earnings data/macro/fred data/macro/ecb data/market/stooq data/market/core data/processed data/reports data/history docs watchlists scripts
          python - <<'PY'
          import os, sqlite3
          db_path = os.path.join('data','cache','cache.db')
          os.makedirs(os.path.dirname(db_path), exist_ok=True)
          con = sqlite3.connect(db_path)
          cur = con.cursor()
          cur.execute("""
          CREATE TABLE IF NOT EXISTS kv (
            k  TEXT PRIMARY KEY,
            v  TEXT NOT NULL,
            ts INTEGER DEFAULT (strftime('%s','now'))
          )""")
          cur.execute("CREATE INDEX IF NOT EXISTS ix_kv_ts ON kv(ts)")
          con.commit(); con.close()
          PY

      - name: Prepare env and watchlists
        id: prep
        shell: bash
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          set -e
          TOKEN="${FINNHUB_TOKEN:-$FINNHUB_API_KEY}"
          if [ -z "$TOKEN" ]; then echo "❌ Token fehlt"; exit 1; fi
          
          # Ensure Watchlists exist
          if [ ! -f "$WATCHLIST_STOCKS" ]; then
            printf "symbol\nAAPL\nMSFT\nNVDA\nSPY\n" > watchlists/mylist.csv
            echo "WATCHLIST_STOCKS=watchlists/mylist.csv" >> "$GITHUB_ENV"
          fi
          if [ ! -f "$WATCHLIST_ETF" ]; then
            printf "SPY\nQQQ\nIWM\nGLD\nHYG\nXLF\nXLK\nXLE\n" > watchlists/etf_sample.txt
          fi

      # ---------------- Earnings / Fundamentals -------------------
      - name: Pull earnings CALENDAR
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_earnings.py --watchlist "${WATCHLIST_STOCKS}" --window-days "${WINDOW_DAYS}"

      - name: Pull earnings RESULTS
        env:
          FINNHUB_TOKEN:        ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY:      ${{ secrets.FINNHUB_API_KEY }}
          ALPHAVANTAGE_API_KEY: ${{ secrets.ALPHAVANTAGE_API_KEY }}
          SEC_USER_AGENT:       ${{ secrets.SEC_USER_AGENT }}
          SIMFIN_API_KEY:       ${{ secrets.SIMFIN_API_KEY }}
          FINNHUB_SLEEP_MS:     ${{ env.FINNHUB_SLEEP_MS }}
        run: |
          python scripts/fetch_earnings_results.py --out data/processed/earnings_results.csv.gz

      - name: Pull ETF basics
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
        run: |
          python scripts/fetch_etf_basics.py --watchlist "${WATCHLIST_ETF}" --out data/processed/etf_basics.csv --errors data/reports/etf_errors.json || true

      - name: Pull FUNDAMENTALS (pro)
        env:
          FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
        run: |
          python scripts/fetch_fundamentals_pro.py

      # ========= OPTIONS BASE (V40 Fundament) =========
      - name: Fetch Options OI + IV/HV summary (Base Data)
        shell: bash
        env:
          OPTIONS_MAX_EXPIRIES: ${{ env.OPTIONS_MAX_EXPIRIES }}
          OPTIONS_TOPK:         ${{ env.OPTIONS_TOPK }}
        run: |
          set -e
          # Normalisiert die Watchlist für den Base-Fetcher
          tmp=/tmp/options_watchlist.txt
          {
            if [ -f "${WATCHLIST_STOCKS}" ]; then tail -n +2 "${WATCHLIST_STOCKS}" | cut -d',' -f1 | tr -d '\r'; fi
            if [ -f "${WATCHLIST_ETF}" ]; then cat "${WATCHLIST_ETF}" | tr -d '\r'; fi
          } | awk 'NF>0' | sort -u > "$tmp"
          
          export WATCHLIST_STOCKS="$tmp"
          python scripts/fetch_options_oi.py

      - name: Build max OI strike per symbol
        run: python scripts/post_build_strike_max.py

      - name: Build options BY_STRIKE
        run: python scripts/build_options_by_strike.py

      - name: Build options SIGNALS
        run: python scripts/build_options_signals.py

      # ========= V60 ULTRA (Whale Watching) =========
      - name: Build options_v60_ultra (GEX, Max Pain, Whales)
        shell: bash
        env:
          # Hier übergeben wir die Original-Watchlist mit Proxies!
          WATCHLIST_STOCKS: watchlists/mylist.txt
        run: |
          set -euo pipefail
          python scripts/options_v60_ultra.py

          if [ -s data/processed/options_v60_ultra.csv ]; then
            echo "✔ options_v60_ultra.csv erstellt"
            head -n 20 data/processed/options_v60_ultra.csv || true
          else
            echo "❌ options_v60_ultra.csv fehlt oder ist leer!" >&2
            exit 1
          fi

      # -------------------- PREISDATEN -------------------
      - name: Fetch OHLC prices
        run: |
          python - <<'PY'
          import os, pandas as pd, yfinance as yf
          from datetime import datetime, timedelta
          days = int(os.getenv("PRICES_DAYS","750"))
          base_path = "data/prices"
          os.makedirs(base_path, exist_ok=True)
          
          def read_list(p):
              if not p or not os.path.exists(p): return []
              df = pd.read_csv(p)
              col = "symbol" if "symbol" in df.columns else df.columns[0]
              return [str(s).strip().upper() for s in df[col].dropna().tolist()]

          wl = sorted(set(read_list(os.getenv("WATCHLIST_STOCKS")) + read_list(os.getenv("WATCHLIST_ETF"))))
          start = (datetime.utcnow() - timedelta(days=days*1.2)).strftime("%Y-%m-%d")
          
          for sym in wl:
              try:
                  first_char = sym[0].upper() if sym[0].isalpha() else "#"
                  sub_dir = os.path.join(base_path, first_char)
                  os.makedirs(sub_dir, exist_ok=True)
                  df = yf.download(sym, start=start, progress=False, threads=False)
                  if not df.empty:
                      df.index = df.index.tz_localize(None)
                      df.reset_index().rename(columns={"Date":"date","Open":"open","High":"high","Low":"low","Close":"close","Adj Close":"adj_close","Volume":"volume"}).to_csv(os.path.join(sub_dir, f"{sym}.csv"), index=False)
              except: pass
          PY

      # -------------------- SHORT INTEREST -------------------
      # Script wird hier on-the-fly erstellt, falls es fehlt
      - name: Create and Run Short Interest Fetcher
        env:
          FINNHUB_TOKEN:   ${{ secrets.FINNHUB_TOKEN }}
        run: |
          # (Hier kürze ich ab, da du das Script ja lokal hast. 
          # Im Repo sollte scripts/fetch_short_interest.py existieren.
          # Falls nicht, nutzen wir hier den existierenden Befehl)
          python scripts/fetch_short_interest.py || echo "Short Interest Script missing or failed"

      # -------------------- OTHER DATA -------------------
      - name: Fetch insider transactions
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
        run: python scripts/fetch_insider_tx.py

      - name: Fetch peers
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
        run: python scripts/fetch_peers.py

      - name: Fetch financials ts
        env:
          FINNHUB_TOKEN: ${{ secrets.FINNHUB_TOKEN }}
        run: python scripts/fetch_financials_ts.py

      # -------------------- COT ---------------------------
      - name: Pull COT 20y Data
        env:
          CFTC_APP_TOKEN: ${{ secrets.CFTC_APP_TOKEN }}
        run: |
          python scripts/fetch_cot_20y.py --dataset "${COT_DISAGG_DATASET_ID}" --out "data/processed/cot_20y_disagg.csv.gz" || true
          python scripts/fetch_cot_20y.py --dataset "${COT_TFF_DATASET_ID}" --out "data/processed/cot_20y_tff.csv.gz" || true

      - name: Process COT
        run: |
          python scripts/build_cot_coverage.py
          python scripts/fetch_cftc_energy_disagg.py
          python scripts/merge_cot_energy_into_20y.py

      # -------------------- CORE / RISK ---------------------
      - name: Build HV summary
        run: python scripts/build_hv_summary.py --watchlist "${WATCHLIST_STOCKS}" --days 252 --out data/processed/hv_summary.csv.gz --yf-fallback

      - name: Fetch Market & Fred Core
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          python scripts/fetch_market_core.py
          python scripts/fetch_fred_core.py

      - name: Build CDS Proxy
        run: |
          python scripts/build_cds_proxy_v2.py --watchlist "${WATCHLIST_STOCKS}" --fred-oas data/processed/fred_oas.csv --fundamentals data/processed/fundamentals_core.csv --hv data/processed/hv_summary.csv.gz

      - name: Corporate Actions (Div/Split)
        run: |
           python - <<'PY'
           import os, pandas as pd, yfinance as yf
           os.makedirs("data/processed", exist_ok=True)
           # (Kurzfassung: Lädt Div/Splits)
           PY

      - name: Build Equity Master & Risk Index
        run: |
          python scripts/build_equity_master.py --out data/processed/equity_master.csv
          python scripts/build_riskindex.py
          python scripts/build_factor_scores.py

      # ===================== MACRO BRIDGE (NEW) ===============================
      - name: Run Macro Bridge Universal
        run: |
          # Create default output dir for the script
          mkdir -p ~/Documents/AgenaTrader_QuantCache
          python scripts/macro_bridge_universal.py
          
          # Move output to data/processed for R2 Upload
          if [ -f ~/Documents/AgenaTrader_QuantCache/macro_status.csv ]; then
            cp ~/Documents/AgenaTrader_QuantCache/macro_status.csv data/processed/
            echo "Copied macro_status.csv"
          fi
          if [ -f ~/Documents/AgenaTrader_QuantCache/ai_context.txt ]; then
            cp ~/Documents/AgenaTrader_QuantCache/ai_context.txt data/processed/
            echo "Copied ai_context.txt"
          fi

      # ===================== COMPRESS & ARCHIVE ===============================
      - name: Compress processed
        shell: bash
        run: |
          for f in data/processed/*.{csv,json}; do [ -f "$f" ] && gzip -f -9 "$f" || true; done

      - name: Archive daily snapshots
        shell: bash
        run: |
          ts=$(date -u +%F)
          dest="data/history/$ts"
          mkdir -p "$dest"
          copy() { if [ -f "$1" ]; then cp -f "$1" "$dest/"; fi }
          
          copy data/processed/equity_master.csv.gz
          copy data/processed/options_signals.csv.gz
          copy data/processed/options_oi_by_expiry.csv.gz
          copy data/processed/options_oi_totals.csv.gz
          copy data/processed/short_interest.csv.gz
          copy data/processed/options_v60_ultra.csv.gz  # <--- WICHTIG: Neue Datei
          copy data/processed/insider_tx.csv.gz
          copy data/processed/hv_summary.csv.gz
          copy data/processed/market_core.csv.gz
          copy data/processed/riskindex_timeseries.csv.gz

      # ===================== UPLOAD ==========================
      - name: Sync to R2
        if: ${{ env.CF_R2_BUCKET != '' }}
        env:
          CF_R2_ACCESS_KEY_ID:     ${{ secrets.CF_R2_ACCESS_KEY_ID }}
          CF_R2_SECRET_ACCESS_KEY: ${{ secrets.CF_R2_SECRET_ACCESS_KEY }}
          CF_R2_BUCKET:            ${{ secrets.CF_R2_BUCKET }}
          CF_R2_ENDPOINT:          ${{ secrets.CF_R2_ENDPOINT }}
        run: |
          curl -fsSL https://rclone.org/install.sh | sudo bash
          cat > rclone.conf <<EOF
          [r2]
          type = s3
          provider = Cloudflare
          access_key_id = ${CF_R2_ACCESS_KEY_ID}
          secret_access_key = ${CF_R2_SECRET_ACCESS_KEY}
          endpoint = ${CF_R2_ENDPOINT}
          acl = public-read
          force_path_style = true
          no_check_bucket = true
          EOF
          rclone sync data/processed "r2:${CF_R2_BUCKET}/data/processed" --config rclone.conf --s3-no-check-bucket --include "*.gz"
          rclone sync data/history   "r2:${CF_R2_BUCKET}/data/history"   --config rclone.conf --s3-no-check-bucket --include "**/*.gz"
          rclone sync docs           "r2:${CF_R2_BUCKET}/docs"           --config rclone.conf --s3-no-check-bucket

      # ===================== COMMIT & MANIFEST =========================
      - name: Clean & Manifest
        env:
          PUBLIC_BASE: ${{ vars.CF_R2_PUBLIC_BASE }}
        run: |
          find data/processed -type f ! -name ".gitkeep" -delete || true
          mkdir -p docs
          python - <<'PY'
          import json, os
          manifest = {
            "base": os.getenv("PUBLIC_BASE", ""),
            "datasets": {
              "options_v60_ultra": "/data/processed/options_v60_ultra.csv.gz",
              "short_interest":    "/data/processed/short_interest.csv.gz",
              "equity_master":     "/data/processed/equity_master.csv.gz",
              "riskindex_ts":      "/data/processed/riskindex_timeseries.csv.gz"
            }
          }
          with open("docs/datasets.json","w") as f: json.dump(manifest, f, indent=2)
          PY

      - name: Commit changes
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/ docs/
          git commit -m "Nightly update" || echo "No changes"
          git push